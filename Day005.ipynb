{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day005.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM+u5epX0LJlCQVJnPbue8K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaum09/100daysofDLcode/blob/dev/Day005.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0Kvg6AUagd8",
        "colab_type": "text"
      },
      "source": [
        "Import the needed dependencies :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgxzPgyt6m6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "from xgboost import XGBRegressor"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caoiEfC67dkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data():\n",
        "    #get train data\n",
        "    train_data_path ='train.csv'\n",
        "    train = pd.read_csv(train_data_path)\n",
        "    \n",
        "    #get test data\n",
        "    test_data_path ='test.csv'\n",
        "    test = pd.read_csv(test_data_path)\n",
        "    \n",
        "    return train , test\n",
        "\n",
        "def get_combined_data():\n",
        "  #reading train data\n",
        "  train , test = get_data()\n",
        "\n",
        "  target = train.SalePrice\n",
        "  train.drop(['SalePrice'],axis = 1 , inplace = True)\n",
        "\n",
        "  combined = train.append(test)\n",
        "  combined.reset_index(inplace=True)\n",
        "  combined.drop(['index', 'Id'], inplace=True, axis=1)\n",
        "  return combined, target\n",
        "\n",
        "#Load train and test data into pandas DataFrames\n",
        "train_data, test_data = get_data()\n",
        "\n",
        "#Combine train and test data to process them together\n",
        "combined, target = get_combined_data()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2ZoI24n7gqt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "60159d24-14ee-47d2-b887-2208cc747e96"
      },
      "source": [
        "combined.describe()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtFinSF2</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>LowQualFinSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>KitchenAbvGr</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>GarageYrBlt</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2433.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2896.000000</td>\n",
              "      <td>2918.000000</td>\n",
              "      <td>2918.000000</td>\n",
              "      <td>2918.000000</td>\n",
              "      <td>2918.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2917.000000</td>\n",
              "      <td>2917.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2760.000000</td>\n",
              "      <td>2918.000000</td>\n",
              "      <td>2918.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "      <td>2919.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>57.137718</td>\n",
              "      <td>69.305795</td>\n",
              "      <td>10168.114080</td>\n",
              "      <td>6.089072</td>\n",
              "      <td>5.564577</td>\n",
              "      <td>1971.312778</td>\n",
              "      <td>1984.264474</td>\n",
              "      <td>102.201312</td>\n",
              "      <td>441.423235</td>\n",
              "      <td>49.582248</td>\n",
              "      <td>560.772104</td>\n",
              "      <td>1051.777587</td>\n",
              "      <td>1159.581706</td>\n",
              "      <td>336.483727</td>\n",
              "      <td>4.694416</td>\n",
              "      <td>1500.759849</td>\n",
              "      <td>0.429894</td>\n",
              "      <td>0.061364</td>\n",
              "      <td>1.568003</td>\n",
              "      <td>0.380267</td>\n",
              "      <td>2.860226</td>\n",
              "      <td>1.044536</td>\n",
              "      <td>6.451524</td>\n",
              "      <td>0.597122</td>\n",
              "      <td>1978.113406</td>\n",
              "      <td>1.766621</td>\n",
              "      <td>472.874572</td>\n",
              "      <td>93.709832</td>\n",
              "      <td>47.486811</td>\n",
              "      <td>23.098321</td>\n",
              "      <td>2.602261</td>\n",
              "      <td>16.062350</td>\n",
              "      <td>2.251799</td>\n",
              "      <td>50.825968</td>\n",
              "      <td>6.213087</td>\n",
              "      <td>2007.792737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>42.517628</td>\n",
              "      <td>23.344905</td>\n",
              "      <td>7886.996359</td>\n",
              "      <td>1.409947</td>\n",
              "      <td>1.113131</td>\n",
              "      <td>30.291442</td>\n",
              "      <td>20.894344</td>\n",
              "      <td>179.334253</td>\n",
              "      <td>455.610826</td>\n",
              "      <td>169.205611</td>\n",
              "      <td>439.543659</td>\n",
              "      <td>440.766258</td>\n",
              "      <td>392.362079</td>\n",
              "      <td>428.701456</td>\n",
              "      <td>46.396825</td>\n",
              "      <td>506.051045</td>\n",
              "      <td>0.524736</td>\n",
              "      <td>0.245687</td>\n",
              "      <td>0.552969</td>\n",
              "      <td>0.502872</td>\n",
              "      <td>0.822693</td>\n",
              "      <td>0.214462</td>\n",
              "      <td>1.569379</td>\n",
              "      <td>0.646129</td>\n",
              "      <td>25.574285</td>\n",
              "      <td>0.761624</td>\n",
              "      <td>215.394815</td>\n",
              "      <td>126.526589</td>\n",
              "      <td>67.575493</td>\n",
              "      <td>64.244246</td>\n",
              "      <td>25.188169</td>\n",
              "      <td>56.184365</td>\n",
              "      <td>35.663946</td>\n",
              "      <td>567.402211</td>\n",
              "      <td>2.714762</td>\n",
              "      <td>1.314964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>20.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>1300.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1872.000000</td>\n",
              "      <td>1950.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>334.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>334.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1895.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2006.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>20.000000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>7478.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1953.500000</td>\n",
              "      <td>1965.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>220.000000</td>\n",
              "      <td>793.000000</td>\n",
              "      <td>876.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1126.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1960.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>320.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2007.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>50.000000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>9453.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1973.000000</td>\n",
              "      <td>1993.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>368.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>467.000000</td>\n",
              "      <td>989.500000</td>\n",
              "      <td>1082.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1444.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1979.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>480.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2008.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>70.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>11570.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>2001.000000</td>\n",
              "      <td>2004.000000</td>\n",
              "      <td>164.000000</td>\n",
              "      <td>733.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>805.500000</td>\n",
              "      <td>1302.000000</td>\n",
              "      <td>1387.500000</td>\n",
              "      <td>704.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1743.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2002.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>576.000000</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>2009.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>190.000000</td>\n",
              "      <td>313.000000</td>\n",
              "      <td>215245.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>2010.000000</td>\n",
              "      <td>2010.000000</td>\n",
              "      <td>1600.000000</td>\n",
              "      <td>5644.000000</td>\n",
              "      <td>1526.000000</td>\n",
              "      <td>2336.000000</td>\n",
              "      <td>6110.000000</td>\n",
              "      <td>5095.000000</td>\n",
              "      <td>2065.000000</td>\n",
              "      <td>1064.000000</td>\n",
              "      <td>5642.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2207.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1488.000000</td>\n",
              "      <td>1424.000000</td>\n",
              "      <td>742.000000</td>\n",
              "      <td>1012.000000</td>\n",
              "      <td>508.000000</td>\n",
              "      <td>576.000000</td>\n",
              "      <td>800.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>2010.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        MSSubClass  LotFrontage  ...       MoSold       YrSold\n",
              "count  2919.000000  2433.000000  ...  2919.000000  2919.000000\n",
              "mean     57.137718    69.305795  ...     6.213087  2007.792737\n",
              "std      42.517628    23.344905  ...     2.714762     1.314964\n",
              "min      20.000000    21.000000  ...     1.000000  2006.000000\n",
              "25%      20.000000    59.000000  ...     4.000000  2007.000000\n",
              "50%      50.000000    68.000000  ...     6.000000  2008.000000\n",
              "75%      70.000000    80.000000  ...     8.000000  2009.000000\n",
              "max     190.000000   313.000000  ...    12.000000  2010.000000\n",
              "\n",
              "[8 rows x 36 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD8esBZibC2J",
        "colab_type": "text"
      },
      "source": [
        "Define a function to get the columns that don't have any missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrOa8ULC7jcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cols_with_no_nans(df,col_type):\n",
        "    '''\n",
        "    Arguments :\n",
        "    df : The dataframe to process\n",
        "    col_type : \n",
        "          num : to only get numerical columns with no nans\n",
        "          no_num : to only get nun-numerical columns with no nans\n",
        "          all : to get any columns with no nans    \n",
        "    '''\n",
        "    if (col_type == 'num'):\n",
        "        predictors = df.select_dtypes(exclude=['object'])\n",
        "    elif (col_type == 'no_num'):\n",
        "        predictors = df.select_dtypes(include=['object'])\n",
        "    elif (col_type == 'all'):\n",
        "        predictors = df\n",
        "    else :\n",
        "        print('Error : choose a type (num, no_num, all)')\n",
        "        return 0\n",
        "    cols_with_no_nans = []\n",
        "    for col in predictors.columns:\n",
        "        if not df[col].isnull().any():\n",
        "            cols_with_no_nans.append(col)\n",
        "    return cols_with_no_nans"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FuLnowzbKRk",
        "colab_type": "text"
      },
      "source": [
        "Get the columns that do not have any missing values ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syOzshNs7mFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_cols = get_cols_with_no_nans(combined , 'num')\n",
        "cat_cols = get_cols_with_no_nans(combined , 'no_num')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frf_IY8Onvqm",
        "colab_type": "text"
      },
      "source": [
        "Let's see how many columns we got"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Hl4SVg7ov8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9aec5f90-ffbc-4f54-cf16-e947e449f8f9"
      },
      "source": [
        "print ('Number of numerical columns with no nan values :',len(num_cols))\n",
        "print ('Number of nun-numerical columns with no nan values :',len(cat_cols))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of numerical columns with no nan values : 25\n",
            "Number of nun-numerical columns with no nan values : 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmvpDPNk7rJn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "ae1ba2b7-0042-4b9e-e496-8712c4989337"
      },
      "source": [
        "combined = combined[num_cols + cat_cols]\n",
        "combined.hist(figsize = (12,10))\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x720 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N793AWuKeS7v",
        "colab_type": "text"
      },
      "source": [
        "**The correlation between the features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp9t0lqv7tnr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "outputId": "234753b7-e21c-4e1a-83ca-7e304a6b4b5a"
      },
      "source": [
        "train_data = train_data[num_cols + cat_cols]\n",
        "train_data['Target'] = target\n",
        "\n",
        "C_mat = train_data.corr()\n",
        "fig = plt.figure(figsize = (15,15))\n",
        "\n",
        "sb.heatmap(C_mat, vmax = .8, square = True)\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x1080 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2A_bc2tqLf7",
        "colab_type": "text"
      },
      "source": [
        "From the correlation heatmap above, we see that about 15 features are highly correlated with the target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV0ziidQqnE3",
        "colab_type": "text"
      },
      "source": [
        "**One Hot Encode The Categorical Features :**\n",
        "\n",
        "We will encode the categorical features using one hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gJUWRPYcMPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def oneHotEncode(df,colNames):\n",
        "    for col in colNames:\n",
        "        if( df[col].dtype == np.dtype('object')):\n",
        "            dummies = pd.get_dummies(df[col],prefix=col)\n",
        "            df = pd.concat([df,dummies],axis=1)\n",
        "\n",
        "            #drop the encoded column\n",
        "            df.drop([col],axis = 1 , inplace=True)\n",
        "    return df"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95Eo3GpMcQt4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b41edf2b-0704-44ff-fc4c-7c867921e9b8"
      },
      "source": [
        "print('There were {} columns before encoding categorical features'.format(combined.shape[1]))\n",
        "combined = oneHotEncode(combined, cat_cols)\n",
        "print('There are {} columns after encoding categorical features'.format(combined.shape[1]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There were 45 columns before encoding categorical features\n",
            "There are 149 columns after encoding categorical features\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MosRPZPEtEgt",
        "colab_type": "text"
      },
      "source": [
        "Now, split back combined dataFrame to training data and test data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwtNJE2ucQ6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_combined():\n",
        "    global combined\n",
        "    train = combined[:1460]\n",
        "    test = combined[1460:]\n",
        "\n",
        "    return train , test "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqG3IKdFcQxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = split_combined()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPKYTh4wtinr",
        "colab_type": "text"
      },
      "source": [
        "## Second : Make the Deep Neural Network\n",
        " * Define a sequential model\n",
        " * Add some dense layers\n",
        " * Use '**relu**' as the activation function in the hidden layers\n",
        " * Use a '**normal**' initializer as the kernal_intializer \n",
        "           Initializers define the way to set the initial random weights of Keras layers.\n",
        " * We will use mean_absolute_error as a loss function\n",
        " * Define the output layer with only one node\n",
        " * Use 'linear 'as the activation function for the output layer\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFUFvsHJcQlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model = Sequential()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrZVKcMbwCcI",
        "colab_type": "text"
      },
      "source": [
        "**The Input Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xOy0gJzc7Cg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6RW_lRRwG2i",
        "colab_type": "text"
      },
      "source": [
        "**The Hidden Layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyrKt91Pc7JC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHSMp0zJwKRc",
        "colab_type": "text"
      },
      "source": [
        "**The Output Layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERVi2S2Kc7R3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHpg10glxNam",
        "colab_type": "text"
      },
      "source": [
        "**Compile the network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWu9eeHqc7N5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "ed297b20-40da-4aa4-862e-dd9a5403111b"
      },
      "source": [
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 128)               19200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 184,065\n",
            "Trainable params: 184,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMirCFkrxUA8",
        "colab_type": "text"
      },
      "source": [
        "**Define a checkpoint callback :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGulRcpic7F0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYgT8VgWxicp",
        "colab_type": "text"
      },
      "source": [
        "## Third : Train the model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwG_VwHlc69g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cad212cd-93c3-4b7b-af5d-5382f8bee609"
      },
      "source": [
        "NN_model.fit(train, target, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 137638.6875 - mean_absolute_error: 137638.6875\n",
            "Epoch 00001: val_loss improved from inf to 55841.58984, saving model to Weights-001--55841.58984.hdf5\n",
            "37/37 [==============================] - 0s 11ms/step - loss: 137962.5781 - mean_absolute_error: 137962.5781 - val_loss: 55841.5898 - val_mean_absolute_error: 55841.5898\n",
            "Epoch 2/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 53475.3789 - mean_absolute_error: 53475.3789\n",
            "Epoch 00002: val_loss improved from 55841.58984 to 47343.07422, saving model to Weights-002--47343.07422.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 53475.3789 - mean_absolute_error: 53475.3789 - val_loss: 47343.0742 - val_mean_absolute_error: 47343.0742\n",
            "Epoch 3/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 47740.7109 - mean_absolute_error: 47740.7109\n",
            "Epoch 00003: val_loss improved from 47343.07422 to 44790.09766, saving model to Weights-003--44790.09766.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 46673.6875 - mean_absolute_error: 46673.6875 - val_loss: 44790.0977 - val_mean_absolute_error: 44790.0977\n",
            "Epoch 4/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 42180.2695 - mean_absolute_error: 42180.2695\n",
            "Epoch 00004: val_loss improved from 44790.09766 to 42005.08984, saving model to Weights-004--42005.08984.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 42170.0625 - mean_absolute_error: 42170.0625 - val_loss: 42005.0898 - val_mean_absolute_error: 42005.0898\n",
            "Epoch 5/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 37843.3672 - mean_absolute_error: 37843.3672\n",
            "Epoch 00005: val_loss improved from 42005.08984 to 40930.00000, saving model to Weights-005--40930.00000.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 39144.2148 - mean_absolute_error: 39144.2148 - val_loss: 40930.0000 - val_mean_absolute_error: 40930.0000\n",
            "Epoch 6/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 37228.0352 - mean_absolute_error: 37228.0352\n",
            "Epoch 00006: val_loss did not improve from 40930.00000\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 37128.2070 - mean_absolute_error: 37128.2070 - val_loss: 43319.0820 - val_mean_absolute_error: 43319.0820\n",
            "Epoch 7/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 39551.6836 - mean_absolute_error: 39551.6836\n",
            "Epoch 00007: val_loss improved from 40930.00000 to 39012.52734, saving model to Weights-007--39012.52734.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 36838.0977 - mean_absolute_error: 36838.0977 - val_loss: 39012.5273 - val_mean_absolute_error: 39012.5273\n",
            "Epoch 8/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 35463.6562 - mean_absolute_error: 35463.6562\n",
            "Epoch 00008: val_loss improved from 39012.52734 to 37155.77344, saving model to Weights-008--37155.77344.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 34315.0078 - mean_absolute_error: 34315.0078 - val_loss: 37155.7734 - val_mean_absolute_error: 37155.7734\n",
            "Epoch 9/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 35395.6367 - mean_absolute_error: 35395.6367\n",
            "Epoch 00009: val_loss improved from 37155.77344 to 35770.53516, saving model to Weights-009--35770.53516.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 33804.0586 - mean_absolute_error: 33804.0586 - val_loss: 35770.5352 - val_mean_absolute_error: 35770.5352\n",
            "Epoch 10/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 33528.8867 - mean_absolute_error: 33528.8867\n",
            "Epoch 00010: val_loss improved from 35770.53516 to 35487.76562, saving model to Weights-010--35487.76562.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 33037.2891 - mean_absolute_error: 33037.2891 - val_loss: 35487.7656 - val_mean_absolute_error: 35487.7656\n",
            "Epoch 11/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 34828.2539 - mean_absolute_error: 34828.2539\n",
            "Epoch 00011: val_loss did not improve from 35487.76562\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 33375.0117 - mean_absolute_error: 33375.0117 - val_loss: 37873.5664 - val_mean_absolute_error: 37873.5664\n",
            "Epoch 12/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 33369.6484 - mean_absolute_error: 33369.6484\n",
            "Epoch 00012: val_loss improved from 35487.76562 to 35066.49219, saving model to Weights-012--35066.49219.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 32122.3125 - mean_absolute_error: 32122.3125 - val_loss: 35066.4922 - val_mean_absolute_error: 35066.4922\n",
            "Epoch 13/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 30722.8242 - mean_absolute_error: 30722.8242\n",
            "Epoch 00013: val_loss did not improve from 35066.49219\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 31989.8965 - mean_absolute_error: 31989.8965 - val_loss: 37623.2891 - val_mean_absolute_error: 37623.2891\n",
            "Epoch 14/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 32213.9648 - mean_absolute_error: 32213.9648\n",
            "Epoch 00014: val_loss improved from 35066.49219 to 34870.41016, saving model to Weights-014--34870.41016.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 32357.0820 - mean_absolute_error: 32357.0820 - val_loss: 34870.4102 - val_mean_absolute_error: 34870.4102\n",
            "Epoch 15/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 32236.0586 - mean_absolute_error: 32236.0586\n",
            "Epoch 00015: val_loss improved from 34870.41016 to 34785.64062, saving model to Weights-015--34785.64062.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 31862.4551 - mean_absolute_error: 31862.4551 - val_loss: 34785.6406 - val_mean_absolute_error: 34785.6406\n",
            "Epoch 16/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 31475.8828 - mean_absolute_error: 31475.8828\n",
            "Epoch 00016: val_loss did not improve from 34785.64062\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 31423.4102 - mean_absolute_error: 31423.4102 - val_loss: 34861.2734 - val_mean_absolute_error: 34861.2734\n",
            "Epoch 17/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 31969.6855 - mean_absolute_error: 31969.6855\n",
            "Epoch 00017: val_loss improved from 34785.64062 to 34158.48047, saving model to Weights-017--34158.48047.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 31969.6855 - mean_absolute_error: 31969.6855 - val_loss: 34158.4805 - val_mean_absolute_error: 34158.4805\n",
            "Epoch 18/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 31601.0938 - mean_absolute_error: 31601.0938\n",
            "Epoch 00018: val_loss did not improve from 34158.48047\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 31713.0039 - mean_absolute_error: 31713.0039 - val_loss: 34205.7109 - val_mean_absolute_error: 34205.7109\n",
            "Epoch 19/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 31797.6074 - mean_absolute_error: 31797.6074\n",
            "Epoch 00019: val_loss did not improve from 34158.48047\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 31674.7988 - mean_absolute_error: 31674.7988 - val_loss: 34370.8203 - val_mean_absolute_error: 34370.8203\n",
            "Epoch 20/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 31972.8203 - mean_absolute_error: 31972.8203\n",
            "Epoch 00020: val_loss did not improve from 34158.48047\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 31955.7812 - mean_absolute_error: 31955.7812 - val_loss: 34281.8203 - val_mean_absolute_error: 34281.8203\n",
            "Epoch 21/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 31532.3496 - mean_absolute_error: 31532.3496\n",
            "Epoch 00021: val_loss did not improve from 34158.48047\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 31532.3496 - mean_absolute_error: 31532.3496 - val_loss: 39811.8203 - val_mean_absolute_error: 39811.8203\n",
            "Epoch 22/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 31544.8066 - mean_absolute_error: 31544.8066\n",
            "Epoch 00022: val_loss did not improve from 34158.48047\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 31289.3457 - mean_absolute_error: 31289.3457 - val_loss: 35417.7305 - val_mean_absolute_error: 35417.7305\n",
            "Epoch 23/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 30695.5449 - mean_absolute_error: 30695.5449\n",
            "Epoch 00023: val_loss improved from 34158.48047 to 34104.32422, saving model to Weights-023--34104.32422.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 30695.5449 - mean_absolute_error: 30695.5449 - val_loss: 34104.3242 - val_mean_absolute_error: 34104.3242\n",
            "Epoch 24/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 30415.8984 - mean_absolute_error: 30415.8984\n",
            "Epoch 00024: val_loss did not improve from 34104.32422\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 30628.4043 - mean_absolute_error: 30628.4043 - val_loss: 34427.5156 - val_mean_absolute_error: 34427.5156\n",
            "Epoch 25/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 30841.8379 - mean_absolute_error: 30841.8379\n",
            "Epoch 00025: val_loss did not improve from 34104.32422\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 31145.4629 - mean_absolute_error: 31145.4629 - val_loss: 34662.9727 - val_mean_absolute_error: 34662.9727\n",
            "Epoch 26/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 31333.0215 - mean_absolute_error: 31333.0215\n",
            "Epoch 00026: val_loss improved from 34104.32422 to 34035.38281, saving model to Weights-026--34035.38281.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 31333.0215 - mean_absolute_error: 31333.0215 - val_loss: 34035.3828 - val_mean_absolute_error: 34035.3828\n",
            "Epoch 27/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 31145.5273 - mean_absolute_error: 31145.5273\n",
            "Epoch 00027: val_loss did not improve from 34035.38281\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 31145.5273 - mean_absolute_error: 31145.5273 - val_loss: 36900.4102 - val_mean_absolute_error: 36900.4102\n",
            "Epoch 28/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 31078.6641 - mean_absolute_error: 31078.6641\n",
            "Epoch 00028: val_loss improved from 34035.38281 to 34033.67188, saving model to Weights-028--34033.67188.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 31108.8555 - mean_absolute_error: 31108.8555 - val_loss: 34033.6719 - val_mean_absolute_error: 34033.6719\n",
            "Epoch 29/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 30481.6035 - mean_absolute_error: 30481.6035\n",
            "Epoch 00029: val_loss improved from 34033.67188 to 33933.98438, saving model to Weights-029--33933.98438.hdf5\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 30716.3398 - mean_absolute_error: 30716.3398 - val_loss: 33933.9844 - val_mean_absolute_error: 33933.9844\n",
            "Epoch 30/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 32760.6211 - mean_absolute_error: 32760.6211\n",
            "Epoch 00030: val_loss did not improve from 33933.98438\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 32756.7461 - mean_absolute_error: 32756.7461 - val_loss: 36233.5039 - val_mean_absolute_error: 36233.5039\n",
            "Epoch 31/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 32624.4902 - mean_absolute_error: 32624.4902\n",
            "Epoch 00031: val_loss did not improve from 33933.98438\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 32502.9316 - mean_absolute_error: 32502.9316 - val_loss: 33981.2070 - val_mean_absolute_error: 33981.2070\n",
            "Epoch 32/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 30710.0605 - mean_absolute_error: 30710.0605\n",
            "Epoch 00032: val_loss improved from 33933.98438 to 33698.26562, saving model to Weights-032--33698.26562.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 30683.5508 - mean_absolute_error: 30683.5508 - val_loss: 33698.2656 - val_mean_absolute_error: 33698.2656\n",
            "Epoch 33/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 31309.2246 - mean_absolute_error: 31309.2246\n",
            "Epoch 00033: val_loss did not improve from 33698.26562\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 31006.7402 - mean_absolute_error: 31006.7402 - val_loss: 33949.0703 - val_mean_absolute_error: 33949.0703\n",
            "Epoch 34/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 30388.0625 - mean_absolute_error: 30388.0625\n",
            "Epoch 00034: val_loss improved from 33698.26562 to 33670.56250, saving model to Weights-034--33670.56250.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 30388.0625 - mean_absolute_error: 30388.0625 - val_loss: 33670.5625 - val_mean_absolute_error: 33670.5625\n",
            "Epoch 35/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 30686.7227 - mean_absolute_error: 30686.7227\n",
            "Epoch 00035: val_loss improved from 33670.56250 to 33582.10938, saving model to Weights-035--33582.10938.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 30631.9629 - mean_absolute_error: 30631.9629 - val_loss: 33582.1094 - val_mean_absolute_error: 33582.1094\n",
            "Epoch 36/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 30068.7598 - mean_absolute_error: 30068.7598\n",
            "Epoch 00036: val_loss did not improve from 33582.10938\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 29962.8398 - mean_absolute_error: 29962.8398 - val_loss: 33768.8359 - val_mean_absolute_error: 33768.8359\n",
            "Epoch 37/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 29933.6348 - mean_absolute_error: 29933.6348\n",
            "Epoch 00037: val_loss did not improve from 33582.10938\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 29920.1992 - mean_absolute_error: 29920.1992 - val_loss: 37505.5469 - val_mean_absolute_error: 37505.5469\n",
            "Epoch 38/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 31562.1035 - mean_absolute_error: 31562.1035\n",
            "Epoch 00038: val_loss improved from 33582.10938 to 33150.64844, saving model to Weights-038--33150.64844.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 31475.6309 - mean_absolute_error: 31475.6309 - val_loss: 33150.6484 - val_mean_absolute_error: 33150.6484\n",
            "Epoch 39/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 30736.1035 - mean_absolute_error: 30736.1035\n",
            "Epoch 00039: val_loss did not improve from 33150.64844\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 30736.1035 - mean_absolute_error: 30736.1035 - val_loss: 33291.5195 - val_mean_absolute_error: 33291.5195\n",
            "Epoch 40/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 30223.0625 - mean_absolute_error: 30223.0625\n",
            "Epoch 00040: val_loss improved from 33150.64844 to 33149.65234, saving model to Weights-040--33149.65234.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 30223.0625 - mean_absolute_error: 30223.0625 - val_loss: 33149.6523 - val_mean_absolute_error: 33149.6523\n",
            "Epoch 41/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 29518.7578 - mean_absolute_error: 29518.7578\n",
            "Epoch 00041: val_loss improved from 33149.65234 to 32775.25391, saving model to Weights-041--32775.25391.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 29535.8223 - mean_absolute_error: 29535.8223 - val_loss: 32775.2539 - val_mean_absolute_error: 32775.2539\n",
            "Epoch 42/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 30035.5430 - mean_absolute_error: 30035.5430\n",
            "Epoch 00042: val_loss did not improve from 32775.25391\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 29688.4902 - mean_absolute_error: 29688.4902 - val_loss: 33461.7578 - val_mean_absolute_error: 33461.7578\n",
            "Epoch 43/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 30180.0664 - mean_absolute_error: 30180.0664\n",
            "Epoch 00043: val_loss improved from 32775.25391 to 32684.34180, saving model to Weights-043--32684.34180.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 30404.1035 - mean_absolute_error: 30404.1035 - val_loss: 32684.3418 - val_mean_absolute_error: 32684.3418\n",
            "Epoch 44/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 28816.1562 - mean_absolute_error: 28816.1562\n",
            "Epoch 00044: val_loss did not improve from 32684.34180\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 29652.3809 - mean_absolute_error: 29652.3809 - val_loss: 32983.7969 - val_mean_absolute_error: 32983.7969\n",
            "Epoch 45/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 29632.1797 - mean_absolute_error: 29632.1797\n",
            "Epoch 00045: val_loss did not improve from 32684.34180\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 29605.2578 - mean_absolute_error: 29605.2578 - val_loss: 36982.2773 - val_mean_absolute_error: 36982.2773\n",
            "Epoch 46/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 30193.8281 - mean_absolute_error: 30193.8281\n",
            "Epoch 00046: val_loss improved from 32684.34180 to 32281.52344, saving model to Weights-046--32281.52344.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 30193.8281 - mean_absolute_error: 30193.8281 - val_loss: 32281.5234 - val_mean_absolute_error: 32281.5234\n",
            "Epoch 47/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 30709.1523 - mean_absolute_error: 30709.1523\n",
            "Epoch 00047: val_loss did not improve from 32281.52344\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 30699.9922 - mean_absolute_error: 30699.9922 - val_loss: 32639.3145 - val_mean_absolute_error: 32639.3145\n",
            "Epoch 48/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 29374.0879 - mean_absolute_error: 29374.0879\n",
            "Epoch 00048: val_loss did not improve from 32281.52344\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 29471.7715 - mean_absolute_error: 29471.7715 - val_loss: 33451.0352 - val_mean_absolute_error: 33451.0352\n",
            "Epoch 49/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 29540.1426 - mean_absolute_error: 29540.1426\n",
            "Epoch 00049: val_loss did not improve from 32281.52344\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 29637.3145 - mean_absolute_error: 29637.3145 - val_loss: 32926.5469 - val_mean_absolute_error: 32926.5469\n",
            "Epoch 50/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 29557.8320 - mean_absolute_error: 29557.8320\n",
            "Epoch 00050: val_loss did not improve from 32281.52344\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 29557.8320 - mean_absolute_error: 29557.8320 - val_loss: 32790.8164 - val_mean_absolute_error: 32790.8164\n",
            "Epoch 51/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 29948.8125 - mean_absolute_error: 29948.8125\n",
            "Epoch 00051: val_loss did not improve from 32281.52344\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 29863.2051 - mean_absolute_error: 29863.2051 - val_loss: 32707.6406 - val_mean_absolute_error: 32707.6406\n",
            "Epoch 52/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 30176.0273 - mean_absolute_error: 30176.0273\n",
            "Epoch 00052: val_loss did not improve from 32281.52344\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 30176.0273 - mean_absolute_error: 30176.0273 - val_loss: 32811.3750 - val_mean_absolute_error: 32811.3750\n",
            "Epoch 53/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 28797.3027 - mean_absolute_error: 28797.3027\n",
            "Epoch 00053: val_loss did not improve from 32281.52344\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 28908.7363 - mean_absolute_error: 28908.7363 - val_loss: 32346.7852 - val_mean_absolute_error: 32346.7852\n",
            "Epoch 54/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 29326.3887 - mean_absolute_error: 29326.3887\n",
            "Epoch 00054: val_loss did not improve from 32281.52344\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 29852.6465 - mean_absolute_error: 29852.6465 - val_loss: 32890.6016 - val_mean_absolute_error: 32890.6016\n",
            "Epoch 55/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 28578.5879 - mean_absolute_error: 28578.5879\n",
            "Epoch 00055: val_loss improved from 32281.52344 to 32227.64648, saving model to Weights-055--32227.64648.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 28595.4785 - mean_absolute_error: 28595.4785 - val_loss: 32227.6465 - val_mean_absolute_error: 32227.6465\n",
            "Epoch 56/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 28993.2754 - mean_absolute_error: 28993.2754\n",
            "Epoch 00056: val_loss improved from 32227.64648 to 31827.86914, saving model to Weights-056--31827.86914.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 28986.1504 - mean_absolute_error: 28986.1504 - val_loss: 31827.8691 - val_mean_absolute_error: 31827.8691\n",
            "Epoch 57/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 28695.7559 - mean_absolute_error: 28695.7559\n",
            "Epoch 00057: val_loss did not improve from 31827.86914\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 28877.1309 - mean_absolute_error: 28877.1309 - val_loss: 31876.7715 - val_mean_absolute_error: 31876.7715\n",
            "Epoch 58/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 28749.0996 - mean_absolute_error: 28749.0996\n",
            "Epoch 00058: val_loss did not improve from 31827.86914\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 28749.0996 - mean_absolute_error: 28749.0996 - val_loss: 34300.5312 - val_mean_absolute_error: 34300.5312\n",
            "Epoch 59/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 30861.3887 - mean_absolute_error: 30861.3887\n",
            "Epoch 00059: val_loss did not improve from 31827.86914\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 30284.2715 - mean_absolute_error: 30284.2715 - val_loss: 32118.2129 - val_mean_absolute_error: 32118.2129\n",
            "Epoch 60/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 28467.5625 - mean_absolute_error: 28467.5625\n",
            "Epoch 00060: val_loss did not improve from 31827.86914\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 28377.2559 - mean_absolute_error: 28377.2559 - val_loss: 32231.1641 - val_mean_absolute_error: 32231.1641\n",
            "Epoch 61/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 28506.0781 - mean_absolute_error: 28506.0781\n",
            "Epoch 00061: val_loss improved from 31827.86914 to 31618.72852, saving model to Weights-061--31618.72852.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 28506.0781 - mean_absolute_error: 28506.0781 - val_loss: 31618.7285 - val_mean_absolute_error: 31618.7285\n",
            "Epoch 62/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 29645.4961 - mean_absolute_error: 29645.4961\n",
            "Epoch 00062: val_loss did not improve from 31618.72852\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 29037.5000 - mean_absolute_error: 29037.5000 - val_loss: 31954.0508 - val_mean_absolute_error: 31954.0508\n",
            "Epoch 63/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 28976.9668 - mean_absolute_error: 28976.9668\n",
            "Epoch 00063: val_loss did not improve from 31618.72852\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 28457.6738 - mean_absolute_error: 28457.6738 - val_loss: 31910.8086 - val_mean_absolute_error: 31910.8086\n",
            "Epoch 64/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 28640.6191 - mean_absolute_error: 28640.6191\n",
            "Epoch 00064: val_loss did not improve from 31618.72852\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 29122.8906 - mean_absolute_error: 29122.8906 - val_loss: 31736.5488 - val_mean_absolute_error: 31736.5488\n",
            "Epoch 65/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 28337.6973 - mean_absolute_error: 28337.6973\n",
            "Epoch 00065: val_loss improved from 31618.72852 to 31591.58984, saving model to Weights-065--31591.58984.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 28351.5801 - mean_absolute_error: 28351.5801 - val_loss: 31591.5898 - val_mean_absolute_error: 31591.5898\n",
            "Epoch 66/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 29038.7988 - mean_absolute_error: 29038.7988\n",
            "Epoch 00066: val_loss did not improve from 31591.58984\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 29038.7988 - mean_absolute_error: 29038.7988 - val_loss: 31757.5918 - val_mean_absolute_error: 31757.5918\n",
            "Epoch 67/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 28988.6875 - mean_absolute_error: 28988.6875\n",
            "Epoch 00067: val_loss improved from 31591.58984 to 31512.32227, saving model to Weights-067--31512.32227.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 28099.2832 - mean_absolute_error: 28099.2832 - val_loss: 31512.3223 - val_mean_absolute_error: 31512.3223\n",
            "Epoch 68/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 27949.0293 - mean_absolute_error: 27949.0293\n",
            "Epoch 00068: val_loss did not improve from 31512.32227\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 28464.9277 - mean_absolute_error: 28464.9277 - val_loss: 34311.3984 - val_mean_absolute_error: 34311.3984\n",
            "Epoch 69/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 28528.5781 - mean_absolute_error: 28528.5781\n",
            "Epoch 00069: val_loss did not improve from 31512.32227\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 28302.0527 - mean_absolute_error: 28302.0527 - val_loss: 36501.2148 - val_mean_absolute_error: 36501.2148\n",
            "Epoch 70/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 28486.4805 - mean_absolute_error: 28486.4805\n",
            "Epoch 00070: val_loss did not improve from 31512.32227\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 28486.4805 - mean_absolute_error: 28486.4805 - val_loss: 31801.7578 - val_mean_absolute_error: 31801.7578\n",
            "Epoch 71/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 28969.2422 - mean_absolute_error: 28969.2422\n",
            "Epoch 00071: val_loss improved from 31512.32227 to 31508.06445, saving model to Weights-071--31508.06445.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 28440.2227 - mean_absolute_error: 28440.2227 - val_loss: 31508.0645 - val_mean_absolute_error: 31508.0645\n",
            "Epoch 72/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 28750.0273 - mean_absolute_error: 28750.0273\n",
            "Epoch 00072: val_loss improved from 31508.06445 to 31051.16016, saving model to Weights-072--31051.16016.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 28750.0273 - mean_absolute_error: 28750.0273 - val_loss: 31051.1602 - val_mean_absolute_error: 31051.1602\n",
            "Epoch 73/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 28428.1895 - mean_absolute_error: 28428.1895\n",
            "Epoch 00073: val_loss improved from 31051.16016 to 30717.47266, saving model to Weights-073--30717.47266.hdf5\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 28281.4199 - mean_absolute_error: 28281.4199 - val_loss: 30717.4727 - val_mean_absolute_error: 30717.4727\n",
            "Epoch 74/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 27614.1973 - mean_absolute_error: 27614.1973\n",
            "Epoch 00074: val_loss improved from 30717.47266 to 30208.60938, saving model to Weights-074--30208.60938.hdf5\n",
            "37/37 [==============================] - 0s 5ms/step - loss: 27546.8398 - mean_absolute_error: 27546.8398 - val_loss: 30208.6094 - val_mean_absolute_error: 30208.6094\n",
            "Epoch 75/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 27211.3984 - mean_absolute_error: 27211.3984\n",
            "Epoch 00075: val_loss did not improve from 30208.60938\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 27071.5000 - mean_absolute_error: 27071.5000 - val_loss: 30749.4922 - val_mean_absolute_error: 30749.4922\n",
            "Epoch 76/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 27211.5801 - mean_absolute_error: 27211.5801\n",
            "Epoch 00076: val_loss did not improve from 30208.60938\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 27582.4434 - mean_absolute_error: 27582.4434 - val_loss: 30980.7988 - val_mean_absolute_error: 30980.7988\n",
            "Epoch 77/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 28135.9512 - mean_absolute_error: 28135.9512\n",
            "Epoch 00077: val_loss did not improve from 30208.60938\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 28043.4512 - mean_absolute_error: 28043.4512 - val_loss: 30637.6875 - val_mean_absolute_error: 30637.6875\n",
            "Epoch 78/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 27306.8496 - mean_absolute_error: 27306.8496\n",
            "Epoch 00078: val_loss did not improve from 30208.60938\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 27509.3613 - mean_absolute_error: 27509.3613 - val_loss: 30472.6367 - val_mean_absolute_error: 30472.6367\n",
            "Epoch 79/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 27545.1191 - mean_absolute_error: 27545.1191\n",
            "Epoch 00079: val_loss did not improve from 30208.60938\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 27200.0527 - mean_absolute_error: 27200.0527 - val_loss: 30765.0508 - val_mean_absolute_error: 30765.0508\n",
            "Epoch 80/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 26840.5020 - mean_absolute_error: 26840.5020\n",
            "Epoch 00080: val_loss did not improve from 30208.60938\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 26678.8359 - mean_absolute_error: 26678.8359 - val_loss: 30658.1680 - val_mean_absolute_error: 30658.1680\n",
            "Epoch 81/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 27444.3516 - mean_absolute_error: 27444.3516\n",
            "Epoch 00081: val_loss improved from 30208.60938 to 30073.06836, saving model to Weights-081--30073.06836.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 27310.0781 - mean_absolute_error: 27310.0781 - val_loss: 30073.0684 - val_mean_absolute_error: 30073.0684\n",
            "Epoch 82/500\n",
            "33/37 [=========================>....] - ETA: 0s - loss: 27670.8047 - mean_absolute_error: 27670.8047\n",
            "Epoch 00082: val_loss improved from 30073.06836 to 29759.39062, saving model to Weights-082--29759.39062.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 27720.8066 - mean_absolute_error: 27720.8066 - val_loss: 29759.3906 - val_mean_absolute_error: 29759.3906\n",
            "Epoch 83/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 26224.5371 - mean_absolute_error: 26224.5371\n",
            "Epoch 00083: val_loss did not improve from 29759.39062\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 26237.8809 - mean_absolute_error: 26237.8809 - val_loss: 30593.9785 - val_mean_absolute_error: 30593.9785\n",
            "Epoch 84/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 26715.5098 - mean_absolute_error: 26715.5098\n",
            "Epoch 00084: val_loss did not improve from 29759.39062\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 26595.0078 - mean_absolute_error: 26595.0078 - val_loss: 31003.0898 - val_mean_absolute_error: 31003.0898\n",
            "Epoch 85/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 26794.5254 - mean_absolute_error: 26794.5254\n",
            "Epoch 00085: val_loss did not improve from 29759.39062\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 27279.1113 - mean_absolute_error: 27279.1113 - val_loss: 33205.5234 - val_mean_absolute_error: 33205.5234\n",
            "Epoch 86/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 26993.0156 - mean_absolute_error: 26993.0156\n",
            "Epoch 00086: val_loss did not improve from 29759.39062\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 27037.3555 - mean_absolute_error: 27037.3555 - val_loss: 30623.4551 - val_mean_absolute_error: 30623.4551\n",
            "Epoch 87/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 25663.2031 - mean_absolute_error: 25663.2031\n",
            "Epoch 00087: val_loss improved from 29759.39062 to 28864.46289, saving model to Weights-087--28864.46289.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 25884.5449 - mean_absolute_error: 25884.5449 - val_loss: 28864.4629 - val_mean_absolute_error: 28864.4629\n",
            "Epoch 88/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 24190.3379 - mean_absolute_error: 24190.3379\n",
            "Epoch 00088: val_loss did not improve from 28864.46289\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 25279.0215 - mean_absolute_error: 25279.0215 - val_loss: 30677.6582 - val_mean_absolute_error: 30677.6582\n",
            "Epoch 89/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 26217.6250 - mean_absolute_error: 26217.6250\n",
            "Epoch 00089: val_loss did not improve from 28864.46289\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 25768.4648 - mean_absolute_error: 25768.4648 - val_loss: 30021.0488 - val_mean_absolute_error: 30021.0488\n",
            "Epoch 90/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 25235.6309 - mean_absolute_error: 25235.6309\n",
            "Epoch 00090: val_loss did not improve from 28864.46289\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 25586.8730 - mean_absolute_error: 25586.8730 - val_loss: 32470.1055 - val_mean_absolute_error: 32470.1055\n",
            "Epoch 91/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 25409.0957 - mean_absolute_error: 25409.0957\n",
            "Epoch 00091: val_loss improved from 28864.46289 to 28700.66211, saving model to Weights-091--28700.66211.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 25217.0488 - mean_absolute_error: 25217.0488 - val_loss: 28700.6621 - val_mean_absolute_error: 28700.6621\n",
            "Epoch 92/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 25719.7539 - mean_absolute_error: 25719.7539\n",
            "Epoch 00092: val_loss did not improve from 28700.66211\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 24982.9922 - mean_absolute_error: 24982.9922 - val_loss: 31473.4785 - val_mean_absolute_error: 31473.4785\n",
            "Epoch 93/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 26814.0078 - mean_absolute_error: 26814.0078\n",
            "Epoch 00093: val_loss improved from 28700.66211 to 28040.34766, saving model to Weights-093--28040.34766.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 26750.3398 - mean_absolute_error: 26750.3398 - val_loss: 28040.3477 - val_mean_absolute_error: 28040.3477\n",
            "Epoch 94/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 23727.8418 - mean_absolute_error: 23727.8418\n",
            "Epoch 00094: val_loss did not improve from 28040.34766\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 24845.8828 - mean_absolute_error: 24845.8828 - val_loss: 28046.4922 - val_mean_absolute_error: 28046.4922\n",
            "Epoch 95/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 23538.6152 - mean_absolute_error: 23538.6152\n",
            "Epoch 00095: val_loss did not improve from 28040.34766\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 24059.9980 - mean_absolute_error: 24059.9980 - val_loss: 28092.9570 - val_mean_absolute_error: 28092.9570\n",
            "Epoch 96/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 24195.1465 - mean_absolute_error: 24195.1465\n",
            "Epoch 00096: val_loss did not improve from 28040.34766\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 24353.8867 - mean_absolute_error: 24353.8867 - val_loss: 30514.9688 - val_mean_absolute_error: 30514.9688\n",
            "Epoch 97/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 25684.6504 - mean_absolute_error: 25684.6504\n",
            "Epoch 00097: val_loss did not improve from 28040.34766\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 25373.2539 - mean_absolute_error: 25373.2539 - val_loss: 28979.5352 - val_mean_absolute_error: 28979.5352\n",
            "Epoch 98/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 24456.0918 - mean_absolute_error: 24456.0918\n",
            "Epoch 00098: val_loss improved from 28040.34766 to 27890.57617, saving model to Weights-098--27890.57617.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 24686.6211 - mean_absolute_error: 24686.6211 - val_loss: 27890.5762 - val_mean_absolute_error: 27890.5762\n",
            "Epoch 99/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 24195.6484 - mean_absolute_error: 24195.6484\n",
            "Epoch 00099: val_loss improved from 27890.57617 to 27619.22656, saving model to Weights-099--27619.22656.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 23953.0977 - mean_absolute_error: 23953.0977 - val_loss: 27619.2266 - val_mean_absolute_error: 27619.2266\n",
            "Epoch 100/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 23017.3809 - mean_absolute_error: 23017.3809\n",
            "Epoch 00100: val_loss improved from 27619.22656 to 26990.13281, saving model to Weights-100--26990.13281.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 23411.4062 - mean_absolute_error: 23411.4062 - val_loss: 26990.1328 - val_mean_absolute_error: 26990.1328\n",
            "Epoch 101/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 24408.1094 - mean_absolute_error: 24408.1094\n",
            "Epoch 00101: val_loss did not improve from 26990.13281\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 25105.4219 - mean_absolute_error: 25105.4219 - val_loss: 35095.3438 - val_mean_absolute_error: 35095.3438\n",
            "Epoch 102/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 25258.9219 - mean_absolute_error: 25258.9219\n",
            "Epoch 00102: val_loss did not improve from 26990.13281\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 26026.5703 - mean_absolute_error: 26026.5703 - val_loss: 28765.6465 - val_mean_absolute_error: 28765.6465\n",
            "Epoch 103/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 24457.3555 - mean_absolute_error: 24457.3555\n",
            "Epoch 00103: val_loss did not improve from 26990.13281\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 23786.9355 - mean_absolute_error: 23786.9355 - val_loss: 27070.2520 - val_mean_absolute_error: 27070.2520\n",
            "Epoch 104/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 24280.3398 - mean_absolute_error: 24280.3398\n",
            "Epoch 00104: val_loss improved from 26990.13281 to 26623.03711, saving model to Weights-104--26623.03711.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 23380.5156 - mean_absolute_error: 23380.5156 - val_loss: 26623.0371 - val_mean_absolute_error: 26623.0371\n",
            "Epoch 105/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 25551.0332 - mean_absolute_error: 25551.0332\n",
            "Epoch 00105: val_loss did not improve from 26623.03711\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 24167.8867 - mean_absolute_error: 24167.8867 - val_loss: 27792.8926 - val_mean_absolute_error: 27792.8926\n",
            "Epoch 106/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 22678.3047 - mean_absolute_error: 22678.3047\n",
            "Epoch 00106: val_loss improved from 26623.03711 to 26332.53906, saving model to Weights-106--26332.53906.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 22798.0488 - mean_absolute_error: 22798.0488 - val_loss: 26332.5391 - val_mean_absolute_error: 26332.5391\n",
            "Epoch 107/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 23171.8320 - mean_absolute_error: 23171.8320\n",
            "Epoch 00107: val_loss did not improve from 26332.53906\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 23338.8672 - mean_absolute_error: 23338.8672 - val_loss: 26770.5156 - val_mean_absolute_error: 26770.5156\n",
            "Epoch 108/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 22483.5371 - mean_absolute_error: 22483.5371\n",
            "Epoch 00108: val_loss did not improve from 26332.53906\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 23654.8867 - mean_absolute_error: 23654.8867 - val_loss: 27370.8164 - val_mean_absolute_error: 27370.8164\n",
            "Epoch 109/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 23954.3418 - mean_absolute_error: 23954.3418\n",
            "Epoch 00109: val_loss did not improve from 26332.53906\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 24413.0586 - mean_absolute_error: 24413.0586 - val_loss: 27425.4238 - val_mean_absolute_error: 27425.4238\n",
            "Epoch 110/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 22251.4355 - mean_absolute_error: 22251.4355\n",
            "Epoch 00110: val_loss improved from 26332.53906 to 25792.22461, saving model to Weights-110--25792.22461.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 22126.8125 - mean_absolute_error: 22126.8125 - val_loss: 25792.2246 - val_mean_absolute_error: 25792.2246\n",
            "Epoch 111/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 21709.9512 - mean_absolute_error: 21709.9512\n",
            "Epoch 00111: val_loss did not improve from 25792.22461\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 21713.9648 - mean_absolute_error: 21713.9648 - val_loss: 26271.4375 - val_mean_absolute_error: 26271.4375\n",
            "Epoch 112/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 22925.8848 - mean_absolute_error: 22925.8848\n",
            "Epoch 00112: val_loss did not improve from 25792.22461\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 22547.6074 - mean_absolute_error: 22547.6074 - val_loss: 26786.9062 - val_mean_absolute_error: 26786.9062\n",
            "Epoch 113/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 22038.4238 - mean_absolute_error: 22038.4238\n",
            "Epoch 00113: val_loss did not improve from 25792.22461\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 21768.2891 - mean_absolute_error: 21768.2891 - val_loss: 26421.2422 - val_mean_absolute_error: 26421.2422\n",
            "Epoch 114/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 22175.4023 - mean_absolute_error: 22175.4023\n",
            "Epoch 00114: val_loss did not improve from 25792.22461\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 22817.8516 - mean_absolute_error: 22817.8516 - val_loss: 26410.2754 - val_mean_absolute_error: 26410.2754\n",
            "Epoch 115/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 23755.3418 - mean_absolute_error: 23755.3418\n",
            "Epoch 00115: val_loss did not improve from 25792.22461\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 23571.6328 - mean_absolute_error: 23571.6328 - val_loss: 27167.6641 - val_mean_absolute_error: 27167.6641\n",
            "Epoch 116/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 23317.1133 - mean_absolute_error: 23317.1133\n",
            "Epoch 00116: val_loss improved from 25792.22461 to 25626.21484, saving model to Weights-116--25626.21484.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 22121.6367 - mean_absolute_error: 22121.6367 - val_loss: 25626.2148 - val_mean_absolute_error: 25626.2148\n",
            "Epoch 117/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 21122.9863 - mean_absolute_error: 21122.9863\n",
            "Epoch 00117: val_loss did not improve from 25626.21484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 21122.9863 - mean_absolute_error: 21122.9863 - val_loss: 27049.8828 - val_mean_absolute_error: 27049.8828\n",
            "Epoch 118/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 21325.0039 - mean_absolute_error: 21325.0039\n",
            "Epoch 00118: val_loss did not improve from 25626.21484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 21057.4004 - mean_absolute_error: 21057.4004 - val_loss: 25773.9922 - val_mean_absolute_error: 25773.9922\n",
            "Epoch 119/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 20586.4629 - mean_absolute_error: 20586.4629\n",
            "Epoch 00119: val_loss improved from 25626.21484 to 25545.85742, saving model to Weights-119--25545.85742.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 20881.2812 - mean_absolute_error: 20881.2812 - val_loss: 25545.8574 - val_mean_absolute_error: 25545.8574\n",
            "Epoch 120/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 22282.7070 - mean_absolute_error: 22282.7070\n",
            "Epoch 00120: val_loss improved from 25545.85742 to 25010.92383, saving model to Weights-120--25010.92383.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 21761.5078 - mean_absolute_error: 21761.5078 - val_loss: 25010.9238 - val_mean_absolute_error: 25010.9238\n",
            "Epoch 121/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 20685.7383 - mean_absolute_error: 20685.7383\n",
            "Epoch 00121: val_loss did not improve from 25010.92383\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 20685.7383 - mean_absolute_error: 20685.7383 - val_loss: 25446.4570 - val_mean_absolute_error: 25446.4570\n",
            "Epoch 122/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 20413.6270 - mean_absolute_error: 20413.6270\n",
            "Epoch 00122: val_loss improved from 25010.92383 to 24443.76172, saving model to Weights-122--24443.76172.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 22153.1602 - mean_absolute_error: 22153.1602 - val_loss: 24443.7617 - val_mean_absolute_error: 24443.7617\n",
            "Epoch 123/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 20505.2520 - mean_absolute_error: 20505.2520\n",
            "Epoch 00123: val_loss did not improve from 24443.76172\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 21649.8672 - mean_absolute_error: 21649.8672 - val_loss: 27367.1992 - val_mean_absolute_error: 27367.1992\n",
            "Epoch 124/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 20050.1855 - mean_absolute_error: 20050.1855\n",
            "Epoch 00124: val_loss did not improve from 24443.76172\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 21438.8281 - mean_absolute_error: 21438.8281 - val_loss: 25225.5156 - val_mean_absolute_error: 25225.5156\n",
            "Epoch 125/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 20782.5371 - mean_absolute_error: 20782.5371\n",
            "Epoch 00125: val_loss did not improve from 24443.76172\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 21377.8965 - mean_absolute_error: 21377.8965 - val_loss: 26353.2148 - val_mean_absolute_error: 26353.2148\n",
            "Epoch 126/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 20472.1914 - mean_absolute_error: 20472.1914\n",
            "Epoch 00126: val_loss did not improve from 24443.76172\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 21316.1230 - mean_absolute_error: 21316.1230 - val_loss: 28645.2910 - val_mean_absolute_error: 28645.2910\n",
            "Epoch 127/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 21729.1367 - mean_absolute_error: 21729.1367\n",
            "Epoch 00127: val_loss did not improve from 24443.76172\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 20591.1992 - mean_absolute_error: 20591.1992 - val_loss: 25936.7949 - val_mean_absolute_error: 25936.7949\n",
            "Epoch 128/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 20638.0293 - mean_absolute_error: 20638.0293\n",
            "Epoch 00128: val_loss did not improve from 24443.76172\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 20391.2402 - mean_absolute_error: 20391.2402 - val_loss: 26246.9922 - val_mean_absolute_error: 26246.9922\n",
            "Epoch 129/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 21187.3809 - mean_absolute_error: 21187.3809\n",
            "Epoch 00129: val_loss did not improve from 24443.76172\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 22602.0039 - mean_absolute_error: 22602.0039 - val_loss: 25955.0195 - val_mean_absolute_error: 25955.0195\n",
            "Epoch 130/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 19284.8750 - mean_absolute_error: 19284.8750\n",
            "Epoch 00130: val_loss did not improve from 24443.76172\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 21225.5254 - mean_absolute_error: 21225.5254 - val_loss: 25840.7695 - val_mean_absolute_error: 25840.7695\n",
            "Epoch 131/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 22021.5527 - mean_absolute_error: 22021.5527\n",
            "Epoch 00131: val_loss did not improve from 24443.76172\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 22262.7754 - mean_absolute_error: 22262.7754 - val_loss: 25324.2148 - val_mean_absolute_error: 25324.2148\n",
            "Epoch 132/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 20125.2500 - mean_absolute_error: 20125.2500\n",
            "Epoch 00132: val_loss did not improve from 24443.76172\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 20634.6934 - mean_absolute_error: 20634.6934 - val_loss: 26410.1406 - val_mean_absolute_error: 26410.1406\n",
            "Epoch 133/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 20487.4316 - mean_absolute_error: 20487.4316\n",
            "Epoch 00133: val_loss did not improve from 24443.76172\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 20818.4531 - mean_absolute_error: 20818.4531 - val_loss: 24731.3496 - val_mean_absolute_error: 24731.3496\n",
            "Epoch 134/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 18823.3262 - mean_absolute_error: 18823.3262\n",
            "Epoch 00134: val_loss improved from 24443.76172 to 24166.59570, saving model to Weights-134--24166.59570.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 19910.8750 - mean_absolute_error: 19910.8750 - val_loss: 24166.5957 - val_mean_absolute_error: 24166.5957\n",
            "Epoch 135/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 20448.4473 - mean_absolute_error: 20448.4473\n",
            "Epoch 00135: val_loss improved from 24166.59570 to 23942.58984, saving model to Weights-135--23942.58984.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 20452.9805 - mean_absolute_error: 20452.9805 - val_loss: 23942.5898 - val_mean_absolute_error: 23942.5898\n",
            "Epoch 136/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 20882.3203 - mean_absolute_error: 20882.3203\n",
            "Epoch 00136: val_loss did not improve from 23942.58984\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 20766.1465 - mean_absolute_error: 20766.1465 - val_loss: 24431.7402 - val_mean_absolute_error: 24431.7402\n",
            "Epoch 137/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 20478.8691 - mean_absolute_error: 20478.8691\n",
            "Epoch 00137: val_loss did not improve from 23942.58984\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 20678.4238 - mean_absolute_error: 20678.4238 - val_loss: 27580.6875 - val_mean_absolute_error: 27580.6875\n",
            "Epoch 138/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 20766.9316 - mean_absolute_error: 20766.9316\n",
            "Epoch 00138: val_loss did not improve from 23942.58984\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 20766.9316 - mean_absolute_error: 20766.9316 - val_loss: 25561.9941 - val_mean_absolute_error: 25561.9941\n",
            "Epoch 139/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 18898.2402 - mean_absolute_error: 18898.2402\n",
            "Epoch 00139: val_loss did not improve from 23942.58984\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19862.7559 - mean_absolute_error: 19862.7559 - val_loss: 24267.4238 - val_mean_absolute_error: 24267.4238\n",
            "Epoch 140/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 20243.6934 - mean_absolute_error: 20243.6934\n",
            "Epoch 00140: val_loss did not improve from 23942.58984\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19946.1133 - mean_absolute_error: 19946.1133 - val_loss: 27829.0840 - val_mean_absolute_error: 27829.0840\n",
            "Epoch 141/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 19005.6406 - mean_absolute_error: 19005.6406\n",
            "Epoch 00141: val_loss improved from 23942.58984 to 23435.97266, saving model to Weights-141--23435.97266.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 19935.6035 - mean_absolute_error: 19935.6035 - val_loss: 23435.9727 - val_mean_absolute_error: 23435.9727\n",
            "Epoch 142/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 20552.6250 - mean_absolute_error: 20552.6250\n",
            "Epoch 00142: val_loss did not improve from 23435.97266\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 22483.4863 - mean_absolute_error: 22483.4863 - val_loss: 24991.6270 - val_mean_absolute_error: 24991.6270\n",
            "Epoch 143/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 21484.0801 - mean_absolute_error: 21484.0801\n",
            "Epoch 00143: val_loss did not improve from 23435.97266\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 21484.0801 - mean_absolute_error: 21484.0801 - val_loss: 27535.9453 - val_mean_absolute_error: 27535.9453\n",
            "Epoch 144/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 20880.3027 - mean_absolute_error: 20880.3027\n",
            "Epoch 00144: val_loss did not improve from 23435.97266\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 21320.5430 - mean_absolute_error: 21320.5430 - val_loss: 27098.7422 - val_mean_absolute_error: 27098.7422\n",
            "Epoch 145/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 21071.4766 - mean_absolute_error: 21071.4766\n",
            "Epoch 00145: val_loss did not improve from 23435.97266\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 20912.8730 - mean_absolute_error: 20912.8730 - val_loss: 28616.3262 - val_mean_absolute_error: 28616.3262\n",
            "Epoch 146/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 20397.8184 - mean_absolute_error: 20397.8184\n",
            "Epoch 00146: val_loss improved from 23435.97266 to 23034.97656, saving model to Weights-146--23034.97656.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 20914.6309 - mean_absolute_error: 20914.6309 - val_loss: 23034.9766 - val_mean_absolute_error: 23034.9766\n",
            "Epoch 147/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 20917.8184 - mean_absolute_error: 20917.8184\n",
            "Epoch 00147: val_loss did not improve from 23034.97656\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 20815.4355 - mean_absolute_error: 20815.4355 - val_loss: 29598.0645 - val_mean_absolute_error: 29598.0645\n",
            "Epoch 148/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 20453.0996 - mean_absolute_error: 20453.0996\n",
            "Epoch 00148: val_loss did not improve from 23034.97656\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19773.1074 - mean_absolute_error: 19773.1074 - val_loss: 26560.7246 - val_mean_absolute_error: 26560.7246\n",
            "Epoch 149/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 21200.1895 - mean_absolute_error: 21200.1895\n",
            "Epoch 00149: val_loss did not improve from 23034.97656\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 20504.1426 - mean_absolute_error: 20504.1426 - val_loss: 25333.0176 - val_mean_absolute_error: 25333.0176\n",
            "Epoch 150/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 22616.1523 - mean_absolute_error: 22616.1523\n",
            "Epoch 00150: val_loss did not improve from 23034.97656\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 22680.1992 - mean_absolute_error: 22680.1992 - val_loss: 26100.0273 - val_mean_absolute_error: 26100.0273\n",
            "Epoch 151/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 20068.6816 - mean_absolute_error: 20068.6816\n",
            "Epoch 00151: val_loss did not improve from 23034.97656\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 19986.4668 - mean_absolute_error: 19986.4668 - val_loss: 23205.7227 - val_mean_absolute_error: 23205.7227\n",
            "Epoch 152/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 19874.1504 - mean_absolute_error: 19874.1504\n",
            "Epoch 00152: val_loss did not improve from 23034.97656\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19128.4355 - mean_absolute_error: 19128.4355 - val_loss: 26349.6230 - val_mean_absolute_error: 26349.6230\n",
            "Epoch 153/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 19833.3887 - mean_absolute_error: 19833.3887\n",
            "Epoch 00153: val_loss improved from 23034.97656 to 23023.24805, saving model to Weights-153--23023.24805.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 19783.5117 - mean_absolute_error: 19783.5117 - val_loss: 23023.2480 - val_mean_absolute_error: 23023.2480\n",
            "Epoch 154/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 21284.9180 - mean_absolute_error: 21284.9180\n",
            "Epoch 00154: val_loss did not improve from 23023.24805\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 21792.8926 - mean_absolute_error: 21792.8926 - val_loss: 24710.0254 - val_mean_absolute_error: 24710.0254\n",
            "Epoch 155/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 20223.7852 - mean_absolute_error: 20223.7852\n",
            "Epoch 00155: val_loss did not improve from 23023.24805\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 20188.5703 - mean_absolute_error: 20188.5703 - val_loss: 23748.7188 - val_mean_absolute_error: 23748.7188\n",
            "Epoch 156/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 20276.8633 - mean_absolute_error: 20276.8633\n",
            "Epoch 00156: val_loss did not improve from 23023.24805\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19418.2051 - mean_absolute_error: 19418.2051 - val_loss: 24420.6816 - val_mean_absolute_error: 24420.6816\n",
            "Epoch 157/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 19870.7852 - mean_absolute_error: 19870.7852\n",
            "Epoch 00157: val_loss did not improve from 23023.24805\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19130.0742 - mean_absolute_error: 19130.0742 - val_loss: 23994.1328 - val_mean_absolute_error: 23994.1328\n",
            "Epoch 158/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 19325.6914 - mean_absolute_error: 19325.6914\n",
            "Epoch 00158: val_loss did not improve from 23023.24805\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19585.2480 - mean_absolute_error: 19585.2480 - val_loss: 24573.6816 - val_mean_absolute_error: 24573.6816\n",
            "Epoch 159/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 19062.9785 - mean_absolute_error: 19062.9785\n",
            "Epoch 00159: val_loss did not improve from 23023.24805\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18868.8125 - mean_absolute_error: 18868.8125 - val_loss: 24038.3281 - val_mean_absolute_error: 24038.3281\n",
            "Epoch 160/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 18875.3477 - mean_absolute_error: 18875.3477\n",
            "Epoch 00160: val_loss did not improve from 23023.24805\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19278.8379 - mean_absolute_error: 19278.8379 - val_loss: 23330.1504 - val_mean_absolute_error: 23330.1504\n",
            "Epoch 161/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 19623.6094 - mean_absolute_error: 19623.6094\n",
            "Epoch 00161: val_loss did not improve from 23023.24805\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18724.5371 - mean_absolute_error: 18724.5371 - val_loss: 24373.7324 - val_mean_absolute_error: 24373.7324\n",
            "Epoch 162/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 18162.3613 - mean_absolute_error: 18162.3613\n",
            "Epoch 00162: val_loss did not improve from 23023.24805\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18203.8535 - mean_absolute_error: 18203.8535 - val_loss: 23955.2227 - val_mean_absolute_error: 23955.2227\n",
            "Epoch 163/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 19440.2109 - mean_absolute_error: 19440.2109\n",
            "Epoch 00163: val_loss did not improve from 23023.24805\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19923.1582 - mean_absolute_error: 19923.1582 - val_loss: 24818.0508 - val_mean_absolute_error: 24818.0508\n",
            "Epoch 164/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 18090.1543 - mean_absolute_error: 18090.1543\n",
            "Epoch 00164: val_loss improved from 23023.24805 to 22769.49609, saving model to Weights-164--22769.49609.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 18467.8359 - mean_absolute_error: 18467.8359 - val_loss: 22769.4961 - val_mean_absolute_error: 22769.4961\n",
            "Epoch 165/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 19918.0078 - mean_absolute_error: 19918.0078\n",
            "Epoch 00165: val_loss did not improve from 22769.49609\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18795.8398 - mean_absolute_error: 18795.8398 - val_loss: 25067.6348 - val_mean_absolute_error: 25067.6348\n",
            "Epoch 166/500\n",
            "18/37 [=============>................] - ETA: 0s - loss: 20969.6055 - mean_absolute_error: 20969.6055\n",
            "Epoch 00166: val_loss did not improve from 22769.49609\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19741.6660 - mean_absolute_error: 19741.6660 - val_loss: 24154.2988 - val_mean_absolute_error: 24154.2988\n",
            "Epoch 167/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 19125.5039 - mean_absolute_error: 19125.5039\n",
            "Epoch 00167: val_loss improved from 22769.49609 to 22404.50391, saving model to Weights-167--22404.50391.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 18634.3848 - mean_absolute_error: 18634.3848 - val_loss: 22404.5039 - val_mean_absolute_error: 22404.5039\n",
            "Epoch 168/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 19054.4082 - mean_absolute_error: 19054.4082\n",
            "Epoch 00168: val_loss did not improve from 22404.50391\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18906.8555 - mean_absolute_error: 18906.8555 - val_loss: 25707.4883 - val_mean_absolute_error: 25707.4883\n",
            "Epoch 169/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 18704.8281 - mean_absolute_error: 18704.8281\n",
            "Epoch 00169: val_loss did not improve from 22404.50391\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18894.9219 - mean_absolute_error: 18894.9219 - val_loss: 24472.2715 - val_mean_absolute_error: 24472.2715\n",
            "Epoch 170/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17554.4023 - mean_absolute_error: 17554.4023\n",
            "Epoch 00170: val_loss did not improve from 22404.50391\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18978.7070 - mean_absolute_error: 18978.7070 - val_loss: 24343.3438 - val_mean_absolute_error: 24343.3438\n",
            "Epoch 171/500\n",
            "18/37 [=============>................] - ETA: 0s - loss: 17467.3984 - mean_absolute_error: 17467.3984\n",
            "Epoch 00171: val_loss improved from 22404.50391 to 22302.89453, saving model to Weights-171--22302.89453.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 18404.0527 - mean_absolute_error: 18404.0527 - val_loss: 22302.8945 - val_mean_absolute_error: 22302.8945\n",
            "Epoch 172/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 19065.0742 - mean_absolute_error: 19065.0742\n",
            "Epoch 00172: val_loss did not improve from 22302.89453\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18882.5742 - mean_absolute_error: 18882.5742 - val_loss: 23029.9238 - val_mean_absolute_error: 23029.9238\n",
            "Epoch 173/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 17948.7383 - mean_absolute_error: 17948.7383\n",
            "Epoch 00173: val_loss did not improve from 22302.89453\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 18204.0996 - mean_absolute_error: 18204.0996 - val_loss: 22688.4805 - val_mean_absolute_error: 22688.4805\n",
            "Epoch 174/500\n",
            "27/37 [====================>.........] - ETA: 0s - loss: 20600.3340 - mean_absolute_error: 20600.3340\n",
            "Epoch 00174: val_loss did not improve from 22302.89453\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 20332.7207 - mean_absolute_error: 20332.7207 - val_loss: 22693.0449 - val_mean_absolute_error: 22693.0449\n",
            "Epoch 175/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 18518.0156 - mean_absolute_error: 18518.0156\n",
            "Epoch 00175: val_loss did not improve from 22302.89453\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18486.7168 - mean_absolute_error: 18486.7168 - val_loss: 22342.5859 - val_mean_absolute_error: 22342.5859\n",
            "Epoch 176/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 18548.1504 - mean_absolute_error: 18548.1504\n",
            "Epoch 00176: val_loss did not improve from 22302.89453\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18769.0332 - mean_absolute_error: 18769.0332 - val_loss: 24776.9629 - val_mean_absolute_error: 24776.9629\n",
            "Epoch 177/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 19230.6406 - mean_absolute_error: 19230.6406\n",
            "Epoch 00177: val_loss did not improve from 22302.89453\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18860.7793 - mean_absolute_error: 18860.7793 - val_loss: 25131.4531 - val_mean_absolute_error: 25131.4531\n",
            "Epoch 178/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 19066.2363 - mean_absolute_error: 19066.2363\n",
            "Epoch 00178: val_loss did not improve from 22302.89453\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18822.4316 - mean_absolute_error: 18822.4316 - val_loss: 24592.0625 - val_mean_absolute_error: 24592.0625\n",
            "Epoch 179/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 18360.4531 - mean_absolute_error: 18360.4531\n",
            "Epoch 00179: val_loss did not improve from 22302.89453\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19276.2031 - mean_absolute_error: 19276.2031 - val_loss: 26819.2773 - val_mean_absolute_error: 26819.2773\n",
            "Epoch 180/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 21420.0547 - mean_absolute_error: 21420.0547\n",
            "Epoch 00180: val_loss did not improve from 22302.89453\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 20688.3965 - mean_absolute_error: 20688.3965 - val_loss: 22815.3535 - val_mean_absolute_error: 22815.3535\n",
            "Epoch 181/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 17972.7852 - mean_absolute_error: 17972.7852\n",
            "Epoch 00181: val_loss did not improve from 22302.89453\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 17983.0488 - mean_absolute_error: 17983.0488 - val_loss: 26893.7383 - val_mean_absolute_error: 26893.7383\n",
            "Epoch 182/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 18802.5215 - mean_absolute_error: 18802.5215\n",
            "Epoch 00182: val_loss did not improve from 22302.89453\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19501.2188 - mean_absolute_error: 19501.2188 - val_loss: 23447.2832 - val_mean_absolute_error: 23447.2832\n",
            "Epoch 183/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 20647.0781 - mean_absolute_error: 20647.0781\n",
            "Epoch 00183: val_loss did not improve from 22302.89453\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19615.6172 - mean_absolute_error: 19615.6172 - val_loss: 22999.7578 - val_mean_absolute_error: 22999.7578\n",
            "Epoch 184/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 20446.8730 - mean_absolute_error: 20446.8730\n",
            "Epoch 00184: val_loss did not improve from 22302.89453\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19021.2500 - mean_absolute_error: 19021.2500 - val_loss: 22520.4805 - val_mean_absolute_error: 22520.4805\n",
            "Epoch 185/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 18755.4473 - mean_absolute_error: 18755.4473\n",
            "Epoch 00185: val_loss did not improve from 22302.89453\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 18813.5605 - mean_absolute_error: 18813.5605 - val_loss: 23173.6465 - val_mean_absolute_error: 23173.6465\n",
            "Epoch 186/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 16938.4805 - mean_absolute_error: 16938.4805\n",
            "Epoch 00186: val_loss did not improve from 22302.89453\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19264.9160 - mean_absolute_error: 19264.9160 - val_loss: 22881.3340 - val_mean_absolute_error: 22881.3340\n",
            "Epoch 187/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 18190.4160 - mean_absolute_error: 18190.4160\n",
            "Epoch 00187: val_loss improved from 22302.89453 to 22003.17578, saving model to Weights-187--22003.17578.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 17689.9922 - mean_absolute_error: 17689.9922 - val_loss: 22003.1758 - val_mean_absolute_error: 22003.1758\n",
            "Epoch 188/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 17592.3047 - mean_absolute_error: 17592.3047\n",
            "Epoch 00188: val_loss did not improve from 22003.17578\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 17739.0176 - mean_absolute_error: 17739.0176 - val_loss: 23260.6914 - val_mean_absolute_error: 23260.6914\n",
            "Epoch 189/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 18241.5391 - mean_absolute_error: 18241.5391\n",
            "Epoch 00189: val_loss did not improve from 22003.17578\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18748.3359 - mean_absolute_error: 18748.3359 - val_loss: 27785.6992 - val_mean_absolute_error: 27785.6992\n",
            "Epoch 190/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 18578.1191 - mean_absolute_error: 18578.1191\n",
            "Epoch 00190: val_loss improved from 22003.17578 to 21350.34180, saving model to Weights-190--21350.34180.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 19435.7715 - mean_absolute_error: 19435.7715 - val_loss: 21350.3418 - val_mean_absolute_error: 21350.3418\n",
            "Epoch 191/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 19652.4102 - mean_absolute_error: 19652.4102\n",
            "Epoch 00191: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19449.0156 - mean_absolute_error: 19449.0156 - val_loss: 21376.3711 - val_mean_absolute_error: 21376.3711\n",
            "Epoch 192/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 18683.3340 - mean_absolute_error: 18683.3340\n",
            "Epoch 00192: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19248.6016 - mean_absolute_error: 19248.6016 - val_loss: 22588.1191 - val_mean_absolute_error: 22588.1191\n",
            "Epoch 193/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17524.0469 - mean_absolute_error: 17524.0469\n",
            "Epoch 00193: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18589.3828 - mean_absolute_error: 18589.3828 - val_loss: 25271.0332 - val_mean_absolute_error: 25271.0332\n",
            "Epoch 194/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 19843.9727 - mean_absolute_error: 19843.9727\n",
            "Epoch 00194: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 20622.0488 - mean_absolute_error: 20622.0488 - val_loss: 22330.2383 - val_mean_absolute_error: 22330.2383\n",
            "Epoch 195/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 18843.2891 - mean_absolute_error: 18843.2891\n",
            "Epoch 00195: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17915.1797 - mean_absolute_error: 17915.1797 - val_loss: 27096.4414 - val_mean_absolute_error: 27096.4414\n",
            "Epoch 196/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 20301.7363 - mean_absolute_error: 20301.7363\n",
            "Epoch 00196: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 20283.2676 - mean_absolute_error: 20283.2676 - val_loss: 24358.9258 - val_mean_absolute_error: 24358.9258\n",
            "Epoch 197/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 20120.1816 - mean_absolute_error: 20120.1816\n",
            "Epoch 00197: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19842.3535 - mean_absolute_error: 19842.3535 - val_loss: 24694.7910 - val_mean_absolute_error: 24694.7910\n",
            "Epoch 198/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 20902.3359 - mean_absolute_error: 20902.3359\n",
            "Epoch 00198: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 20548.7188 - mean_absolute_error: 20548.7188 - val_loss: 24499.7852 - val_mean_absolute_error: 24499.7852\n",
            "Epoch 199/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 19223.2188 - mean_absolute_error: 19223.2188\n",
            "Epoch 00199: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18534.9355 - mean_absolute_error: 18534.9355 - val_loss: 21742.4668 - val_mean_absolute_error: 21742.4668\n",
            "Epoch 200/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 18721.3398 - mean_absolute_error: 18721.3398\n",
            "Epoch 00200: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17695.7480 - mean_absolute_error: 17695.7480 - val_loss: 21795.3887 - val_mean_absolute_error: 21795.3887\n",
            "Epoch 201/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 19051.3164 - mean_absolute_error: 19051.3164\n",
            "Epoch 00201: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18811.2910 - mean_absolute_error: 18811.2910 - val_loss: 24872.7500 - val_mean_absolute_error: 24872.7500\n",
            "Epoch 202/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 20277.8711 - mean_absolute_error: 20277.8711\n",
            "Epoch 00202: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 20790.6172 - mean_absolute_error: 20790.6172 - val_loss: 22266.8633 - val_mean_absolute_error: 22266.8633\n",
            "Epoch 203/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16250.6162 - mean_absolute_error: 16250.6162\n",
            "Epoch 00203: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17511.2227 - mean_absolute_error: 17511.2227 - val_loss: 22556.7070 - val_mean_absolute_error: 22556.7070\n",
            "Epoch 204/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 18617.4023 - mean_absolute_error: 18617.4023\n",
            "Epoch 00204: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17592.0352 - mean_absolute_error: 17592.0352 - val_loss: 22371.8496 - val_mean_absolute_error: 22371.8496\n",
            "Epoch 205/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17474.0273 - mean_absolute_error: 17474.0273\n",
            "Epoch 00205: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17386.3125 - mean_absolute_error: 17386.3125 - val_loss: 21659.1055 - val_mean_absolute_error: 21659.1055\n",
            "Epoch 206/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 20743.3965 - mean_absolute_error: 20743.3965\n",
            "Epoch 00206: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 20776.0391 - mean_absolute_error: 20776.0391 - val_loss: 22386.2852 - val_mean_absolute_error: 22386.2852\n",
            "Epoch 207/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 22802.5215 - mean_absolute_error: 22802.5215\n",
            "Epoch 00207: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 20843.7012 - mean_absolute_error: 20843.7012 - val_loss: 21900.6953 - val_mean_absolute_error: 21900.6953\n",
            "Epoch 208/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17805.4941 - mean_absolute_error: 17805.4941\n",
            "Epoch 00208: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17384.3301 - mean_absolute_error: 17384.3301 - val_loss: 22977.7539 - val_mean_absolute_error: 22977.7539\n",
            "Epoch 209/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 18747.2910 - mean_absolute_error: 18747.2910\n",
            "Epoch 00209: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18114.1367 - mean_absolute_error: 18114.1367 - val_loss: 29624.1172 - val_mean_absolute_error: 29624.1172\n",
            "Epoch 210/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17889.3457 - mean_absolute_error: 17889.3457\n",
            "Epoch 00210: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18464.4531 - mean_absolute_error: 18464.4531 - val_loss: 24537.8281 - val_mean_absolute_error: 24537.8281\n",
            "Epoch 211/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 19193.2363 - mean_absolute_error: 19193.2363\n",
            "Epoch 00211: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19099.7500 - mean_absolute_error: 19099.7500 - val_loss: 23534.4062 - val_mean_absolute_error: 23534.4062\n",
            "Epoch 212/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 17381.0820 - mean_absolute_error: 17381.0820\n",
            "Epoch 00212: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17381.0820 - mean_absolute_error: 17381.0820 - val_loss: 22229.7402 - val_mean_absolute_error: 22229.7402\n",
            "Epoch 213/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 18432.5332 - mean_absolute_error: 18432.5332\n",
            "Epoch 00213: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17432.4219 - mean_absolute_error: 17432.4219 - val_loss: 21447.3477 - val_mean_absolute_error: 21447.3477\n",
            "Epoch 214/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 18059.4258 - mean_absolute_error: 18059.4258\n",
            "Epoch 00214: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17944.0703 - mean_absolute_error: 17944.0703 - val_loss: 22803.7168 - val_mean_absolute_error: 22803.7168\n",
            "Epoch 215/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16847.5723 - mean_absolute_error: 16847.5723\n",
            "Epoch 00215: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 16847.5723 - mean_absolute_error: 16847.5723 - val_loss: 22032.3262 - val_mean_absolute_error: 22032.3262\n",
            "Epoch 216/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 16791.5684 - mean_absolute_error: 16791.5684\n",
            "Epoch 00216: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 16794.6367 - mean_absolute_error: 16794.6367 - val_loss: 26645.6895 - val_mean_absolute_error: 26645.6895\n",
            "Epoch 217/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17141.0957 - mean_absolute_error: 17141.0957\n",
            "Epoch 00217: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17889.7793 - mean_absolute_error: 17889.7793 - val_loss: 21574.6035 - val_mean_absolute_error: 21574.6035\n",
            "Epoch 218/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 19281.3105 - mean_absolute_error: 19281.3105\n",
            "Epoch 00218: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 19332.7305 - mean_absolute_error: 19332.7305 - val_loss: 21843.1289 - val_mean_absolute_error: 21843.1289\n",
            "Epoch 219/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 23820.4902 - mean_absolute_error: 23820.4902\n",
            "Epoch 00219: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 23593.8887 - mean_absolute_error: 23593.8887 - val_loss: 26123.0898 - val_mean_absolute_error: 26123.0898\n",
            "Epoch 220/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 18845.4668 - mean_absolute_error: 18845.4668\n",
            "Epoch 00220: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 18748.3984 - mean_absolute_error: 18748.3984 - val_loss: 21513.8535 - val_mean_absolute_error: 21513.8535\n",
            "Epoch 221/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17549.5996 - mean_absolute_error: 17549.5996\n",
            "Epoch 00221: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17931.9785 - mean_absolute_error: 17931.9785 - val_loss: 23439.9238 - val_mean_absolute_error: 23439.9238\n",
            "Epoch 222/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17239.8770 - mean_absolute_error: 17239.8770\n",
            "Epoch 00222: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17295.6191 - mean_absolute_error: 17295.6191 - val_loss: 21355.9336 - val_mean_absolute_error: 21355.9336\n",
            "Epoch 223/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16429.7031 - mean_absolute_error: 16429.7031\n",
            "Epoch 00223: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16684.5879 - mean_absolute_error: 16684.5879 - val_loss: 22795.5059 - val_mean_absolute_error: 22795.5059\n",
            "Epoch 224/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17082.8984 - mean_absolute_error: 17082.8984\n",
            "Epoch 00224: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16759.0059 - mean_absolute_error: 16759.0059 - val_loss: 22362.0723 - val_mean_absolute_error: 22362.0723\n",
            "Epoch 225/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16106.5732 - mean_absolute_error: 16106.5732\n",
            "Epoch 00225: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17566.2832 - mean_absolute_error: 17566.2832 - val_loss: 21686.6602 - val_mean_absolute_error: 21686.6602\n",
            "Epoch 226/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16598.8320 - mean_absolute_error: 16598.8320\n",
            "Epoch 00226: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17070.8066 - mean_absolute_error: 17070.8066 - val_loss: 22973.7168 - val_mean_absolute_error: 22973.7168\n",
            "Epoch 227/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17418.6367 - mean_absolute_error: 17418.6367\n",
            "Epoch 00227: val_loss did not improve from 21350.34180\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17240.1328 - mean_absolute_error: 17240.1328 - val_loss: 21504.7324 - val_mean_absolute_error: 21504.7324\n",
            "Epoch 228/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 19782.8965 - mean_absolute_error: 19782.8965\n",
            "Epoch 00228: val_loss improved from 21350.34180 to 21279.23828, saving model to Weights-228--21279.23828.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 19075.0762 - mean_absolute_error: 19075.0762 - val_loss: 21279.2383 - val_mean_absolute_error: 21279.2383\n",
            "Epoch 229/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17113.0801 - mean_absolute_error: 17113.0801\n",
            "Epoch 00229: val_loss did not improve from 21279.23828\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17722.5137 - mean_absolute_error: 17722.5137 - val_loss: 23579.4434 - val_mean_absolute_error: 23579.4434\n",
            "Epoch 230/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 16387.7539 - mean_absolute_error: 16387.7539\n",
            "Epoch 00230: val_loss did not improve from 21279.23828\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18609.5410 - mean_absolute_error: 18609.5410 - val_loss: 22433.4004 - val_mean_absolute_error: 22433.4004\n",
            "Epoch 231/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 19380.2793 - mean_absolute_error: 19380.2793\n",
            "Epoch 00231: val_loss did not improve from 21279.23828\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18723.9902 - mean_absolute_error: 18723.9902 - val_loss: 26520.4355 - val_mean_absolute_error: 26520.4355\n",
            "Epoch 232/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 18425.5156 - mean_absolute_error: 18425.5156\n",
            "Epoch 00232: val_loss did not improve from 21279.23828\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18094.1445 - mean_absolute_error: 18094.1445 - val_loss: 22369.7891 - val_mean_absolute_error: 22369.7891\n",
            "Epoch 233/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 18703.9590 - mean_absolute_error: 18703.9590\n",
            "Epoch 00233: val_loss did not improve from 21279.23828\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18129.4258 - mean_absolute_error: 18129.4258 - val_loss: 23907.1270 - val_mean_absolute_error: 23907.1270\n",
            "Epoch 234/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17238.7773 - mean_absolute_error: 17238.7773\n",
            "Epoch 00234: val_loss did not improve from 21279.23828\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17504.7891 - mean_absolute_error: 17504.7891 - val_loss: 22986.9199 - val_mean_absolute_error: 22986.9199\n",
            "Epoch 235/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 17073.7266 - mean_absolute_error: 17073.7266\n",
            "Epoch 00235: val_loss did not improve from 21279.23828\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17073.7266 - mean_absolute_error: 17073.7266 - val_loss: 21311.7129 - val_mean_absolute_error: 21311.7129\n",
            "Epoch 236/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17625.6133 - mean_absolute_error: 17625.6133\n",
            "Epoch 00236: val_loss did not improve from 21279.23828\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17620.4043 - mean_absolute_error: 17620.4043 - val_loss: 22033.3242 - val_mean_absolute_error: 22033.3242\n",
            "Epoch 237/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 19761.6172 - mean_absolute_error: 19761.6172\n",
            "Epoch 00237: val_loss did not improve from 21279.23828\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18801.2168 - mean_absolute_error: 18801.2168 - val_loss: 21284.3672 - val_mean_absolute_error: 21284.3672\n",
            "Epoch 238/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 18397.4824 - mean_absolute_error: 18397.4824\n",
            "Epoch 00238: val_loss did not improve from 21279.23828\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 18397.4824 - mean_absolute_error: 18397.4824 - val_loss: 21973.9238 - val_mean_absolute_error: 21973.9238\n",
            "Epoch 239/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 17382.1250 - mean_absolute_error: 17382.1250\n",
            "Epoch 00239: val_loss did not improve from 21279.23828\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17357.8770 - mean_absolute_error: 17357.8770 - val_loss: 21461.9199 - val_mean_absolute_error: 21461.9199\n",
            "Epoch 240/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16799.9902 - mean_absolute_error: 16799.9902\n",
            "Epoch 00240: val_loss improved from 21279.23828 to 21173.72852, saving model to Weights-240--21173.72852.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 16799.9902 - mean_absolute_error: 16799.9902 - val_loss: 21173.7285 - val_mean_absolute_error: 21173.7285\n",
            "Epoch 241/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 18008.6113 - mean_absolute_error: 18008.6113\n",
            "Epoch 00241: val_loss did not improve from 21173.72852\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 18004.7148 - mean_absolute_error: 18004.7148 - val_loss: 21632.3535 - val_mean_absolute_error: 21632.3535\n",
            "Epoch 242/500\n",
            "30/37 [=======================>......] - ETA: 0s - loss: 19286.7324 - mean_absolute_error: 19286.7324\n",
            "Epoch 00242: val_loss did not improve from 21173.72852\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 18928.9824 - mean_absolute_error: 18928.9824 - val_loss: 22898.3906 - val_mean_absolute_error: 22898.3906\n",
            "Epoch 243/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 17618.0762 - mean_absolute_error: 17618.0762\n",
            "Epoch 00243: val_loss did not improve from 21173.72852\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 17864.8164 - mean_absolute_error: 17864.8164 - val_loss: 25923.3770 - val_mean_absolute_error: 25923.3770\n",
            "Epoch 244/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 17254.9531 - mean_absolute_error: 17254.9531\n",
            "Epoch 00244: val_loss improved from 21173.72852 to 20937.00781, saving model to Weights-244--20937.00781.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 17623.3105 - mean_absolute_error: 17623.3105 - val_loss: 20937.0078 - val_mean_absolute_error: 20937.0078\n",
            "Epoch 245/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16915.4414 - mean_absolute_error: 16915.4414\n",
            "Epoch 00245: val_loss did not improve from 20937.00781\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16695.5273 - mean_absolute_error: 16695.5273 - val_loss: 21006.0039 - val_mean_absolute_error: 21006.0039\n",
            "Epoch 246/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16337.4268 - mean_absolute_error: 16337.4268\n",
            "Epoch 00246: val_loss did not improve from 20937.00781\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16583.2324 - mean_absolute_error: 16583.2324 - val_loss: 21517.4648 - val_mean_absolute_error: 21517.4648\n",
            "Epoch 247/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 16517.3379 - mean_absolute_error: 16517.3379\n",
            "Epoch 00247: val_loss improved from 20937.00781 to 20704.66016, saving model to Weights-247--20704.66016.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 16418.7539 - mean_absolute_error: 16418.7539 - val_loss: 20704.6602 - val_mean_absolute_error: 20704.6602\n",
            "Epoch 248/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 16307.5479 - mean_absolute_error: 16307.5479\n",
            "Epoch 00248: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17467.2227 - mean_absolute_error: 17467.2227 - val_loss: 24924.3301 - val_mean_absolute_error: 24924.3301\n",
            "Epoch 249/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17884.7949 - mean_absolute_error: 17884.7949\n",
            "Epoch 00249: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18116.8145 - mean_absolute_error: 18116.8145 - val_loss: 21051.6211 - val_mean_absolute_error: 21051.6211\n",
            "Epoch 250/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16063.3379 - mean_absolute_error: 16063.3379\n",
            "Epoch 00250: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16082.3223 - mean_absolute_error: 16082.3223 - val_loss: 23402.7578 - val_mean_absolute_error: 23402.7578\n",
            "Epoch 251/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16322.8027 - mean_absolute_error: 16322.8027\n",
            "Epoch 00251: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16724.1230 - mean_absolute_error: 16724.1230 - val_loss: 28463.7422 - val_mean_absolute_error: 28463.7422\n",
            "Epoch 252/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 18537.5996 - mean_absolute_error: 18537.5996\n",
            "Epoch 00252: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17739.8535 - mean_absolute_error: 17739.8535 - val_loss: 25326.7090 - val_mean_absolute_error: 25326.7090\n",
            "Epoch 253/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 18816.5820 - mean_absolute_error: 18816.5820\n",
            "Epoch 00253: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17324.2559 - mean_absolute_error: 17324.2559 - val_loss: 22278.0840 - val_mean_absolute_error: 22278.0840\n",
            "Epoch 254/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17125.3848 - mean_absolute_error: 17125.3848\n",
            "Epoch 00254: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16319.0518 - mean_absolute_error: 16319.0518 - val_loss: 21285.9941 - val_mean_absolute_error: 21285.9941\n",
            "Epoch 255/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15949.6758 - mean_absolute_error: 15949.6758\n",
            "Epoch 00255: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16343.1357 - mean_absolute_error: 16343.1357 - val_loss: 21188.3652 - val_mean_absolute_error: 21188.3652\n",
            "Epoch 256/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 17725.2461 - mean_absolute_error: 17725.2461\n",
            "Epoch 00256: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17119.2363 - mean_absolute_error: 17119.2363 - val_loss: 25403.6328 - val_mean_absolute_error: 25403.6328\n",
            "Epoch 257/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16494.8555 - mean_absolute_error: 16494.8555\n",
            "Epoch 00257: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16395.5293 - mean_absolute_error: 16395.5293 - val_loss: 22120.0000 - val_mean_absolute_error: 22120.0000\n",
            "Epoch 258/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16778.3027 - mean_absolute_error: 16778.3027\n",
            "Epoch 00258: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17359.9785 - mean_absolute_error: 17359.9785 - val_loss: 22551.8672 - val_mean_absolute_error: 22551.8672\n",
            "Epoch 259/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 21720.4883 - mean_absolute_error: 21720.4883\n",
            "Epoch 00259: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18974.9453 - mean_absolute_error: 18974.9453 - val_loss: 21115.2148 - val_mean_absolute_error: 21115.2148\n",
            "Epoch 260/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 19364.2754 - mean_absolute_error: 19364.2754\n",
            "Epoch 00260: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18000.8418 - mean_absolute_error: 18000.8418 - val_loss: 21020.7520 - val_mean_absolute_error: 21020.7520\n",
            "Epoch 261/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17793.8750 - mean_absolute_error: 17793.8750\n",
            "Epoch 00261: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16681.2891 - mean_absolute_error: 16681.2891 - val_loss: 22269.1250 - val_mean_absolute_error: 22269.1250\n",
            "Epoch 262/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16922.8340 - mean_absolute_error: 16922.8340\n",
            "Epoch 00262: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18185.6797 - mean_absolute_error: 18185.6797 - val_loss: 25460.0391 - val_mean_absolute_error: 25460.0391\n",
            "Epoch 263/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 21132.6602 - mean_absolute_error: 21132.6602\n",
            "Epoch 00263: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 21132.6602 - mean_absolute_error: 21132.6602 - val_loss: 23254.7578 - val_mean_absolute_error: 23254.7578\n",
            "Epoch 264/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17827.5723 - mean_absolute_error: 17827.5723\n",
            "Epoch 00264: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17396.0273 - mean_absolute_error: 17396.0273 - val_loss: 21830.1836 - val_mean_absolute_error: 21830.1836\n",
            "Epoch 265/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16928.0410 - mean_absolute_error: 16928.0410\n",
            "Epoch 00265: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16928.0410 - mean_absolute_error: 16928.0410 - val_loss: 21360.8281 - val_mean_absolute_error: 21360.8281\n",
            "Epoch 266/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15614.1826 - mean_absolute_error: 15614.1826\n",
            "Epoch 00266: val_loss did not improve from 20704.66016\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16040.8574 - mean_absolute_error: 16040.8574 - val_loss: 22004.8262 - val_mean_absolute_error: 22004.8262\n",
            "Epoch 267/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 18162.7598 - mean_absolute_error: 18162.7598\n",
            "Epoch 00267: val_loss improved from 20704.66016 to 20332.99023, saving model to Weights-267--20332.99023.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 17221.4688 - mean_absolute_error: 17221.4688 - val_loss: 20332.9902 - val_mean_absolute_error: 20332.9902\n",
            "Epoch 268/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17469.6641 - mean_absolute_error: 17469.6641\n",
            "Epoch 00268: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16719.6660 - mean_absolute_error: 16719.6660 - val_loss: 22298.5430 - val_mean_absolute_error: 22298.5430\n",
            "Epoch 269/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16961.8496 - mean_absolute_error: 16961.8496\n",
            "Epoch 00269: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17307.6152 - mean_absolute_error: 17307.6152 - val_loss: 20761.2988 - val_mean_absolute_error: 20761.2988\n",
            "Epoch 270/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16627.0352 - mean_absolute_error: 16627.0352\n",
            "Epoch 00270: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16627.0352 - mean_absolute_error: 16627.0352 - val_loss: 20983.3477 - val_mean_absolute_error: 20983.3477\n",
            "Epoch 271/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17518.2812 - mean_absolute_error: 17518.2812\n",
            "Epoch 00271: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17149.1660 - mean_absolute_error: 17149.1660 - val_loss: 21063.1719 - val_mean_absolute_error: 21063.1719\n",
            "Epoch 272/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15987.8145 - mean_absolute_error: 15987.8145\n",
            "Epoch 00272: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16601.6367 - mean_absolute_error: 16601.6367 - val_loss: 24026.5859 - val_mean_absolute_error: 24026.5859\n",
            "Epoch 273/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 18271.8027 - mean_absolute_error: 18271.8027\n",
            "Epoch 00273: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17625.8613 - mean_absolute_error: 17625.8613 - val_loss: 20802.2910 - val_mean_absolute_error: 20802.2910\n",
            "Epoch 274/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16221.9414 - mean_absolute_error: 16221.9414\n",
            "Epoch 00274: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16221.9414 - mean_absolute_error: 16221.9414 - val_loss: 22248.3555 - val_mean_absolute_error: 22248.3555\n",
            "Epoch 275/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 18143.6445 - mean_absolute_error: 18143.6445\n",
            "Epoch 00275: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17665.7539 - mean_absolute_error: 17665.7539 - val_loss: 20979.5078 - val_mean_absolute_error: 20979.5078\n",
            "Epoch 276/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17457.8789 - mean_absolute_error: 17457.8789\n",
            "Epoch 00276: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16929.2422 - mean_absolute_error: 16929.2422 - val_loss: 24618.6816 - val_mean_absolute_error: 24618.6816\n",
            "Epoch 277/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16326.1094 - mean_absolute_error: 16326.1094\n",
            "Epoch 00277: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16601.7793 - mean_absolute_error: 16601.7793 - val_loss: 22319.1875 - val_mean_absolute_error: 22319.1875\n",
            "Epoch 278/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17755.8223 - mean_absolute_error: 17755.8223\n",
            "Epoch 00278: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16837.0957 - mean_absolute_error: 16837.0957 - val_loss: 20679.4082 - val_mean_absolute_error: 20679.4082\n",
            "Epoch 279/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15143.8096 - mean_absolute_error: 15143.8096\n",
            "Epoch 00279: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16292.1562 - mean_absolute_error: 16292.1562 - val_loss: 21668.1562 - val_mean_absolute_error: 21668.1562\n",
            "Epoch 280/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16046.6768 - mean_absolute_error: 16046.6768\n",
            "Epoch 00280: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16198.2812 - mean_absolute_error: 16198.2812 - val_loss: 21101.4785 - val_mean_absolute_error: 21101.4785\n",
            "Epoch 281/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 17021.3828 - mean_absolute_error: 17021.3828\n",
            "Epoch 00281: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 16979.1992 - mean_absolute_error: 16979.1992 - val_loss: 21245.2461 - val_mean_absolute_error: 21245.2461\n",
            "Epoch 282/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 18888.5430 - mean_absolute_error: 18888.5430\n",
            "Epoch 00282: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18699.7402 - mean_absolute_error: 18699.7402 - val_loss: 22955.6172 - val_mean_absolute_error: 22955.6172\n",
            "Epoch 283/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 18768.9004 - mean_absolute_error: 18768.9004\n",
            "Epoch 00283: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17836.0098 - mean_absolute_error: 17836.0098 - val_loss: 21332.2344 - val_mean_absolute_error: 21332.2344\n",
            "Epoch 284/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17249.9727 - mean_absolute_error: 17249.9727\n",
            "Epoch 00284: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16614.8379 - mean_absolute_error: 16614.8379 - val_loss: 32509.2363 - val_mean_absolute_error: 32509.2363\n",
            "Epoch 285/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 18682.9668 - mean_absolute_error: 18682.9668\n",
            "Epoch 00285: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 18604.2832 - mean_absolute_error: 18604.2832 - val_loss: 22207.5605 - val_mean_absolute_error: 22207.5605\n",
            "Epoch 286/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 19989.2656 - mean_absolute_error: 19989.2656\n",
            "Epoch 00286: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19123.3750 - mean_absolute_error: 19123.3750 - val_loss: 21915.4453 - val_mean_absolute_error: 21915.4453\n",
            "Epoch 287/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17659.1152 - mean_absolute_error: 17659.1152\n",
            "Epoch 00287: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16815.4277 - mean_absolute_error: 16815.4277 - val_loss: 20805.8379 - val_mean_absolute_error: 20805.8379\n",
            "Epoch 288/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16142.2129 - mean_absolute_error: 16142.2129\n",
            "Epoch 00288: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16906.0488 - mean_absolute_error: 16906.0488 - val_loss: 21392.8809 - val_mean_absolute_error: 21392.8809\n",
            "Epoch 289/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16844.9707 - mean_absolute_error: 16844.9707\n",
            "Epoch 00289: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16844.9707 - mean_absolute_error: 16844.9707 - val_loss: 22618.8770 - val_mean_absolute_error: 22618.8770\n",
            "Epoch 290/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 18060.4648 - mean_absolute_error: 18060.4648\n",
            "Epoch 00290: val_loss did not improve from 20332.99023\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18655.9414 - mean_absolute_error: 18655.9414 - val_loss: 24489.4453 - val_mean_absolute_error: 24489.4453\n",
            "Epoch 291/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 18826.2637 - mean_absolute_error: 18826.2637\n",
            "Epoch 00291: val_loss improved from 20332.99023 to 20321.12109, saving model to Weights-291--20321.12109.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 19231.8242 - mean_absolute_error: 19231.8242 - val_loss: 20321.1211 - val_mean_absolute_error: 20321.1211\n",
            "Epoch 292/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16066.3047 - mean_absolute_error: 16066.3047\n",
            "Epoch 00292: val_loss improved from 20321.12109 to 20073.54297, saving model to Weights-292--20073.54297.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15831.8252 - mean_absolute_error: 15831.8252 - val_loss: 20073.5430 - val_mean_absolute_error: 20073.5430\n",
            "Epoch 293/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15945.2949 - mean_absolute_error: 15945.2949\n",
            "Epoch 00293: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 16836.0625 - mean_absolute_error: 16836.0625 - val_loss: 22347.1270 - val_mean_absolute_error: 22347.1270\n",
            "Epoch 294/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16274.1670 - mean_absolute_error: 16274.1670\n",
            "Epoch 00294: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16117.0938 - mean_absolute_error: 16117.0938 - val_loss: 20076.2637 - val_mean_absolute_error: 20076.2637\n",
            "Epoch 295/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14599.4297 - mean_absolute_error: 14599.4297\n",
            "Epoch 00295: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15123.8047 - mean_absolute_error: 15123.8047 - val_loss: 21038.0820 - val_mean_absolute_error: 21038.0820\n",
            "Epoch 296/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 15884.2998 - mean_absolute_error: 15884.2998\n",
            "Epoch 00296: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15884.2998 - mean_absolute_error: 15884.2998 - val_loss: 21353.4551 - val_mean_absolute_error: 21353.4551\n",
            "Epoch 297/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15776.3418 - mean_absolute_error: 15776.3418\n",
            "Epoch 00297: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15870.4590 - mean_absolute_error: 15870.4590 - val_loss: 21383.0098 - val_mean_absolute_error: 21383.0098\n",
            "Epoch 298/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 16291.5889 - mean_absolute_error: 16291.5889\n",
            "Epoch 00298: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16528.7207 - mean_absolute_error: 16528.7207 - val_loss: 22871.8164 - val_mean_absolute_error: 22871.8164\n",
            "Epoch 299/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17942.6602 - mean_absolute_error: 17942.6602\n",
            "Epoch 00299: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17013.5488 - mean_absolute_error: 17013.5488 - val_loss: 20626.3594 - val_mean_absolute_error: 20626.3594\n",
            "Epoch 300/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17855.2578 - mean_absolute_error: 17855.2578\n",
            "Epoch 00300: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16979.9766 - mean_absolute_error: 16979.9766 - val_loss: 21402.1523 - val_mean_absolute_error: 21402.1523\n",
            "Epoch 301/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 15987.8711 - mean_absolute_error: 15987.8711\n",
            "Epoch 00301: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15987.8711 - mean_absolute_error: 15987.8711 - val_loss: 20924.5156 - val_mean_absolute_error: 20924.5156\n",
            "Epoch 302/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15284.5029 - mean_absolute_error: 15284.5029\n",
            "Epoch 00302: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17618.1816 - mean_absolute_error: 17618.1816 - val_loss: 20685.3633 - val_mean_absolute_error: 20685.3633\n",
            "Epoch 303/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16262.3691 - mean_absolute_error: 16262.3691\n",
            "Epoch 00303: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15688.9521 - mean_absolute_error: 15688.9521 - val_loss: 22893.5078 - val_mean_absolute_error: 22893.5078\n",
            "Epoch 304/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 18237.0273 - mean_absolute_error: 18237.0273\n",
            "Epoch 00304: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18539.9570 - mean_absolute_error: 18539.9570 - val_loss: 21163.1738 - val_mean_absolute_error: 21163.1738\n",
            "Epoch 305/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 15845.3359 - mean_absolute_error: 15845.3359\n",
            "Epoch 00305: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15845.3359 - mean_absolute_error: 15845.3359 - val_loss: 21786.7461 - val_mean_absolute_error: 21786.7461\n",
            "Epoch 306/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15968.3203 - mean_absolute_error: 15968.3203\n",
            "Epoch 00306: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15621.8018 - mean_absolute_error: 15621.8018 - val_loss: 20670.0703 - val_mean_absolute_error: 20670.0703\n",
            "Epoch 307/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16051.8574 - mean_absolute_error: 16051.8574\n",
            "Epoch 00307: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17342.8633 - mean_absolute_error: 17342.8633 - val_loss: 21057.3359 - val_mean_absolute_error: 21057.3359\n",
            "Epoch 308/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 15257.4580 - mean_absolute_error: 15257.4580\n",
            "Epoch 00308: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15238.8701 - mean_absolute_error: 15238.8701 - val_loss: 20116.2695 - val_mean_absolute_error: 20116.2695\n",
            "Epoch 309/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 17062.3730 - mean_absolute_error: 17062.3730\n",
            "Epoch 00309: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17062.3730 - mean_absolute_error: 17062.3730 - val_loss: 21050.8359 - val_mean_absolute_error: 21050.8359\n",
            "Epoch 310/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 17534.5801 - mean_absolute_error: 17534.5801\n",
            "Epoch 00310: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 17466.8535 - mean_absolute_error: 17466.8535 - val_loss: 24415.5137 - val_mean_absolute_error: 24415.5137\n",
            "Epoch 311/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17008.8652 - mean_absolute_error: 17008.8652\n",
            "Epoch 00311: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16017.4043 - mean_absolute_error: 16017.4043 - val_loss: 21945.9414 - val_mean_absolute_error: 21945.9414\n",
            "Epoch 312/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 15665.2480 - mean_absolute_error: 15665.2480\n",
            "Epoch 00312: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15702.7588 - mean_absolute_error: 15702.7588 - val_loss: 21376.2012 - val_mean_absolute_error: 21376.2012\n",
            "Epoch 313/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15844.1904 - mean_absolute_error: 15844.1904\n",
            "Epoch 00313: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15288.2930 - mean_absolute_error: 15288.2930 - val_loss: 20698.7480 - val_mean_absolute_error: 20698.7480\n",
            "Epoch 314/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16367.6797 - mean_absolute_error: 16367.6797\n",
            "Epoch 00314: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 16367.6797 - mean_absolute_error: 16367.6797 - val_loss: 22297.5957 - val_mean_absolute_error: 22297.5957\n",
            "Epoch 315/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 15552.7461 - mean_absolute_error: 15552.7461\n",
            "Epoch 00315: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15631.2520 - mean_absolute_error: 15631.2520 - val_loss: 22046.0059 - val_mean_absolute_error: 22046.0059\n",
            "Epoch 316/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16162.5029 - mean_absolute_error: 16162.5029\n",
            "Epoch 00316: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16285.0117 - mean_absolute_error: 16285.0117 - val_loss: 20158.3145 - val_mean_absolute_error: 20158.3145\n",
            "Epoch 317/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16893.8984 - mean_absolute_error: 16893.8984\n",
            "Epoch 00317: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 16893.8984 - mean_absolute_error: 16893.8984 - val_loss: 20690.1211 - val_mean_absolute_error: 20690.1211\n",
            "Epoch 318/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 16489.2422 - mean_absolute_error: 16489.2422\n",
            "Epoch 00318: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 16519.9551 - mean_absolute_error: 16519.9551 - val_loss: 20701.5215 - val_mean_absolute_error: 20701.5215\n",
            "Epoch 319/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 16967.1074 - mean_absolute_error: 16967.1074\n",
            "Epoch 00319: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 16807.5762 - mean_absolute_error: 16807.5762 - val_loss: 20674.8203 - val_mean_absolute_error: 20674.8203\n",
            "Epoch 320/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 16826.2090 - mean_absolute_error: 16826.2090\n",
            "Epoch 00320: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16786.1406 - mean_absolute_error: 16786.1406 - val_loss: 20872.7988 - val_mean_absolute_error: 20872.7988\n",
            "Epoch 321/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16815.3516 - mean_absolute_error: 16815.3516\n",
            "Epoch 00321: val_loss did not improve from 20073.54297\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16310.8594 - mean_absolute_error: 16310.8594 - val_loss: 20601.4648 - val_mean_absolute_error: 20601.4648\n",
            "Epoch 322/500\n",
            "18/37 [=============>................] - ETA: 0s - loss: 14822.7539 - mean_absolute_error: 14822.7539\n",
            "Epoch 00322: val_loss improved from 20073.54297 to 19713.23438, saving model to Weights-322--19713.23438.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15007.8662 - mean_absolute_error: 15007.8662 - val_loss: 19713.2344 - val_mean_absolute_error: 19713.2344\n",
            "Epoch 323/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15503.8438 - mean_absolute_error: 15503.8438\n",
            "Epoch 00323: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15990.3594 - mean_absolute_error: 15990.3594 - val_loss: 21256.6816 - val_mean_absolute_error: 21256.6816\n",
            "Epoch 324/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17566.7383 - mean_absolute_error: 17566.7383\n",
            "Epoch 00324: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17535.4258 - mean_absolute_error: 17535.4258 - val_loss: 22331.6699 - val_mean_absolute_error: 22331.6699\n",
            "Epoch 325/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14661.0312 - mean_absolute_error: 14661.0312\n",
            "Epoch 00325: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14889.6914 - mean_absolute_error: 14889.6914 - val_loss: 23310.3164 - val_mean_absolute_error: 23310.3164\n",
            "Epoch 326/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17533.1426 - mean_absolute_error: 17533.1426\n",
            "Epoch 00326: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16965.4785 - mean_absolute_error: 16965.4785 - val_loss: 20152.9609 - val_mean_absolute_error: 20152.9609\n",
            "Epoch 327/500\n",
            "18/37 [=============>................] - ETA: 0s - loss: 14512.8799 - mean_absolute_error: 14512.8799\n",
            "Epoch 00327: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15865.8066 - mean_absolute_error: 15865.8066 - val_loss: 22137.5996 - val_mean_absolute_error: 22137.5996\n",
            "Epoch 328/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16953.8281 - mean_absolute_error: 16953.8281\n",
            "Epoch 00328: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17046.1895 - mean_absolute_error: 17046.1895 - val_loss: 24185.8477 - val_mean_absolute_error: 24185.8477\n",
            "Epoch 329/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 22461.2969 - mean_absolute_error: 22461.2969\n",
            "Epoch 00329: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 20181.6348 - mean_absolute_error: 20181.6348 - val_loss: 22160.8652 - val_mean_absolute_error: 22160.8652\n",
            "Epoch 330/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 16393.1934 - mean_absolute_error: 16393.1934\n",
            "Epoch 00330: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16530.5449 - mean_absolute_error: 16530.5449 - val_loss: 20698.7402 - val_mean_absolute_error: 20698.7402\n",
            "Epoch 331/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 16670.4141 - mean_absolute_error: 16670.4141\n",
            "Epoch 00331: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16013.7744 - mean_absolute_error: 16013.7744 - val_loss: 20607.2324 - val_mean_absolute_error: 20607.2324\n",
            "Epoch 332/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16390.6816 - mean_absolute_error: 16390.6816\n",
            "Epoch 00332: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15748.1953 - mean_absolute_error: 15748.1953 - val_loss: 21171.1797 - val_mean_absolute_error: 21171.1797\n",
            "Epoch 333/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16541.1348 - mean_absolute_error: 16541.1348\n",
            "Epoch 00333: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15615.8750 - mean_absolute_error: 15615.8750 - val_loss: 20409.9414 - val_mean_absolute_error: 20409.9414\n",
            "Epoch 334/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 14916.3945 - mean_absolute_error: 14916.3945\n",
            "Epoch 00334: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 14924.8594 - mean_absolute_error: 14924.8594 - val_loss: 20241.2715 - val_mean_absolute_error: 20241.2715\n",
            "Epoch 335/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14682.6016 - mean_absolute_error: 14682.6016\n",
            "Epoch 00335: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14953.5479 - mean_absolute_error: 14953.5479 - val_loss: 22279.0898 - val_mean_absolute_error: 22279.0898\n",
            "Epoch 336/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14377.2373 - mean_absolute_error: 14377.2373\n",
            "Epoch 00336: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14813.5684 - mean_absolute_error: 14813.5684 - val_loss: 27286.4199 - val_mean_absolute_error: 27286.4199\n",
            "Epoch 337/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 17288.3711 - mean_absolute_error: 17288.3711\n",
            "Epoch 00337: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16336.4814 - mean_absolute_error: 16336.4814 - val_loss: 20528.9160 - val_mean_absolute_error: 20528.9160\n",
            "Epoch 338/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14923.7783 - mean_absolute_error: 14923.7783\n",
            "Epoch 00338: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15120.4570 - mean_absolute_error: 15120.4570 - val_loss: 19940.0898 - val_mean_absolute_error: 19940.0898\n",
            "Epoch 339/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 18539.8574 - mean_absolute_error: 18539.8574\n",
            "Epoch 00339: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16905.7461 - mean_absolute_error: 16905.7461 - val_loss: 20406.3672 - val_mean_absolute_error: 20406.3672\n",
            "Epoch 340/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15463.9688 - mean_absolute_error: 15463.9688\n",
            "Epoch 00340: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15004.7070 - mean_absolute_error: 15004.7070 - val_loss: 19882.5176 - val_mean_absolute_error: 19882.5176\n",
            "Epoch 341/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17105.8711 - mean_absolute_error: 17105.8711\n",
            "Epoch 00341: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16373.7393 - mean_absolute_error: 16373.7393 - val_loss: 20129.3086 - val_mean_absolute_error: 20129.3086\n",
            "Epoch 342/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15654.2129 - mean_absolute_error: 15654.2129\n",
            "Epoch 00342: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16440.8691 - mean_absolute_error: 16440.8691 - val_loss: 20663.8535 - val_mean_absolute_error: 20663.8535\n",
            "Epoch 343/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 19950.8828 - mean_absolute_error: 19950.8828\n",
            "Epoch 00343: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 19418.0938 - mean_absolute_error: 19418.0938 - val_loss: 21955.5039 - val_mean_absolute_error: 21955.5039\n",
            "Epoch 344/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 18455.5645 - mean_absolute_error: 18455.5645\n",
            "Epoch 00344: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18480.4102 - mean_absolute_error: 18480.4102 - val_loss: 20369.0996 - val_mean_absolute_error: 20369.0996\n",
            "Epoch 345/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14380.0596 - mean_absolute_error: 14380.0596\n",
            "Epoch 00345: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14986.5586 - mean_absolute_error: 14986.5586 - val_loss: 21456.7031 - val_mean_absolute_error: 21456.7031\n",
            "Epoch 346/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14953.4746 - mean_absolute_error: 14953.4746\n",
            "Epoch 00346: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15160.7480 - mean_absolute_error: 15160.7480 - val_loss: 19934.1934 - val_mean_absolute_error: 19934.1934\n",
            "Epoch 347/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16605.8242 - mean_absolute_error: 16605.8242\n",
            "Epoch 00347: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16570.7227 - mean_absolute_error: 16570.7227 - val_loss: 20461.3242 - val_mean_absolute_error: 20461.3242\n",
            "Epoch 348/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14367.3418 - mean_absolute_error: 14367.3418\n",
            "Epoch 00348: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15482.9756 - mean_absolute_error: 15482.9756 - val_loss: 20279.7051 - val_mean_absolute_error: 20279.7051\n",
            "Epoch 349/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15539.5127 - mean_absolute_error: 15539.5127\n",
            "Epoch 00349: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15138.8320 - mean_absolute_error: 15138.8320 - val_loss: 19818.4980 - val_mean_absolute_error: 19818.4980\n",
            "Epoch 350/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14953.3691 - mean_absolute_error: 14953.3691\n",
            "Epoch 00350: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15392.4248 - mean_absolute_error: 15392.4248 - val_loss: 22927.7129 - val_mean_absolute_error: 22927.7129\n",
            "Epoch 351/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16007.5596 - mean_absolute_error: 16007.5596\n",
            "Epoch 00351: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15253.7959 - mean_absolute_error: 15253.7959 - val_loss: 20728.0352 - val_mean_absolute_error: 20728.0352\n",
            "Epoch 352/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14758.0312 - mean_absolute_error: 14758.0312\n",
            "Epoch 00352: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14417.8926 - mean_absolute_error: 14417.8926 - val_loss: 20740.0664 - val_mean_absolute_error: 20740.0664\n",
            "Epoch 353/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14822.6201 - mean_absolute_error: 14822.6201\n",
            "Epoch 00353: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15549.4277 - mean_absolute_error: 15549.4277 - val_loss: 19812.4648 - val_mean_absolute_error: 19812.4648\n",
            "Epoch 354/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 19181.7969 - mean_absolute_error: 19181.7969\n",
            "Epoch 00354: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18762.3203 - mean_absolute_error: 18762.3203 - val_loss: 20240.6035 - val_mean_absolute_error: 20240.6035\n",
            "Epoch 355/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16992.8887 - mean_absolute_error: 16992.8887\n",
            "Epoch 00355: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 16992.8887 - mean_absolute_error: 16992.8887 - val_loss: 20739.0293 - val_mean_absolute_error: 20739.0293\n",
            "Epoch 356/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16166.3340 - mean_absolute_error: 16166.3340\n",
            "Epoch 00356: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16166.3340 - mean_absolute_error: 16166.3340 - val_loss: 20357.0449 - val_mean_absolute_error: 20357.0449\n",
            "Epoch 357/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16438.1562 - mean_absolute_error: 16438.1562\n",
            "Epoch 00357: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16141.2188 - mean_absolute_error: 16141.2188 - val_loss: 19881.8828 - val_mean_absolute_error: 19881.8828\n",
            "Epoch 358/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14833.5166 - mean_absolute_error: 14833.5166\n",
            "Epoch 00358: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15320.9268 - mean_absolute_error: 15320.9268 - val_loss: 22020.3145 - val_mean_absolute_error: 22020.3145\n",
            "Epoch 359/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 16395.9316 - mean_absolute_error: 16395.9316\n",
            "Epoch 00359: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15749.3252 - mean_absolute_error: 15749.3252 - val_loss: 20133.6309 - val_mean_absolute_error: 20133.6309\n",
            "Epoch 360/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15105.4609 - mean_absolute_error: 15105.4609\n",
            "Epoch 00360: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15059.2588 - mean_absolute_error: 15059.2588 - val_loss: 19846.9121 - val_mean_absolute_error: 19846.9121\n",
            "Epoch 361/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14737.4570 - mean_absolute_error: 14737.4570\n",
            "Epoch 00361: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14523.3770 - mean_absolute_error: 14523.3770 - val_loss: 21605.1094 - val_mean_absolute_error: 21605.1094\n",
            "Epoch 362/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16172.4590 - mean_absolute_error: 16172.4590\n",
            "Epoch 00362: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16375.5156 - mean_absolute_error: 16375.5156 - val_loss: 21581.0000 - val_mean_absolute_error: 21581.0000\n",
            "Epoch 363/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 15404.0859 - mean_absolute_error: 15404.0859\n",
            "Epoch 00363: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15337.5547 - mean_absolute_error: 15337.5547 - val_loss: 20681.3340 - val_mean_absolute_error: 20681.3340\n",
            "Epoch 364/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15072.9277 - mean_absolute_error: 15072.9277\n",
            "Epoch 00364: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15522.1846 - mean_absolute_error: 15522.1846 - val_loss: 20198.2988 - val_mean_absolute_error: 20198.2988\n",
            "Epoch 365/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14985.6113 - mean_absolute_error: 14985.6113\n",
            "Epoch 00365: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14924.6562 - mean_absolute_error: 14924.6562 - val_loss: 21665.7773 - val_mean_absolute_error: 21665.7773\n",
            "Epoch 366/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 16327.6299 - mean_absolute_error: 16327.6299\n",
            "Epoch 00366: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 16239.4453 - mean_absolute_error: 16239.4453 - val_loss: 21541.5508 - val_mean_absolute_error: 21541.5508\n",
            "Epoch 367/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 17045.6504 - mean_absolute_error: 17045.6504\n",
            "Epoch 00367: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16565.7695 - mean_absolute_error: 16565.7695 - val_loss: 20959.1270 - val_mean_absolute_error: 20959.1270\n",
            "Epoch 368/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 15003.3320 - mean_absolute_error: 15003.3320\n",
            "Epoch 00368: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15003.3320 - mean_absolute_error: 15003.3320 - val_loss: 20569.5488 - val_mean_absolute_error: 20569.5488\n",
            "Epoch 369/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 15909.2031 - mean_absolute_error: 15909.2031\n",
            "Epoch 00369: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15901.7549 - mean_absolute_error: 15901.7549 - val_loss: 22346.2930 - val_mean_absolute_error: 22346.2930\n",
            "Epoch 370/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16112.4346 - mean_absolute_error: 16112.4346\n",
            "Epoch 00370: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 16112.4346 - mean_absolute_error: 16112.4346 - val_loss: 20781.3145 - val_mean_absolute_error: 20781.3145\n",
            "Epoch 371/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 18645.3008 - mean_absolute_error: 18645.3008\n",
            "Epoch 00371: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 18645.3008 - mean_absolute_error: 18645.3008 - val_loss: 23731.7109 - val_mean_absolute_error: 23731.7109\n",
            "Epoch 372/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15283.5801 - mean_absolute_error: 15283.5801\n",
            "Epoch 00372: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14922.5293 - mean_absolute_error: 14922.5293 - val_loss: 20112.8867 - val_mean_absolute_error: 20112.8867\n",
            "Epoch 373/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16315.5938 - mean_absolute_error: 16315.5938\n",
            "Epoch 00373: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16806.8262 - mean_absolute_error: 16806.8262 - val_loss: 21797.4434 - val_mean_absolute_error: 21797.4434\n",
            "Epoch 374/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14280.1367 - mean_absolute_error: 14280.1367\n",
            "Epoch 00374: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15178.4863 - mean_absolute_error: 15178.4863 - val_loss: 20431.3574 - val_mean_absolute_error: 20431.3574\n",
            "Epoch 375/500\n",
            "22/37 [================>.............] - ETA: 0s - loss: 14482.1689 - mean_absolute_error: 14482.1689\n",
            "Epoch 00375: val_loss did not improve from 19713.23438\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14892.6064 - mean_absolute_error: 14892.6064 - val_loss: 20020.4355 - val_mean_absolute_error: 20020.4355\n",
            "Epoch 376/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14363.7715 - mean_absolute_error: 14363.7715\n",
            "Epoch 00376: val_loss improved from 19713.23438 to 19570.45703, saving model to Weights-376--19570.45703.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 14902.1289 - mean_absolute_error: 14902.1289 - val_loss: 19570.4570 - val_mean_absolute_error: 19570.4570\n",
            "Epoch 377/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 15925.2412 - mean_absolute_error: 15925.2412\n",
            "Epoch 00377: val_loss did not improve from 19570.45703\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15925.2412 - mean_absolute_error: 15925.2412 - val_loss: 22920.9824 - val_mean_absolute_error: 22920.9824\n",
            "Epoch 378/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15247.1514 - mean_absolute_error: 15247.1514\n",
            "Epoch 00378: val_loss did not improve from 19570.45703\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15255.0107 - mean_absolute_error: 15255.0107 - val_loss: 19919.6602 - val_mean_absolute_error: 19919.6602\n",
            "Epoch 379/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15191.2441 - mean_absolute_error: 15191.2441\n",
            "Epoch 00379: val_loss did not improve from 19570.45703\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15786.4316 - mean_absolute_error: 15786.4316 - val_loss: 21513.5137 - val_mean_absolute_error: 21513.5137\n",
            "Epoch 380/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 17014.8359 - mean_absolute_error: 17014.8359\n",
            "Epoch 00380: val_loss did not improve from 19570.45703\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15937.4619 - mean_absolute_error: 15937.4619 - val_loss: 20609.2559 - val_mean_absolute_error: 20609.2559\n",
            "Epoch 381/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14513.7422 - mean_absolute_error: 14513.7422\n",
            "Epoch 00381: val_loss did not improve from 19570.45703\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16570.2871 - mean_absolute_error: 16570.2871 - val_loss: 26610.1348 - val_mean_absolute_error: 26610.1348\n",
            "Epoch 382/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17078.6016 - mean_absolute_error: 17078.6016\n",
            "Epoch 00382: val_loss did not improve from 19570.45703\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15903.3994 - mean_absolute_error: 15903.3994 - val_loss: 22034.0430 - val_mean_absolute_error: 22034.0430\n",
            "Epoch 383/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15949.8232 - mean_absolute_error: 15949.8232\n",
            "Epoch 00383: val_loss did not improve from 19570.45703\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15501.6113 - mean_absolute_error: 15501.6113 - val_loss: 20799.5918 - val_mean_absolute_error: 20799.5918\n",
            "Epoch 384/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15196.9238 - mean_absolute_error: 15196.9238\n",
            "Epoch 00384: val_loss did not improve from 19570.45703\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16228.5938 - mean_absolute_error: 16228.5938 - val_loss: 23842.6641 - val_mean_absolute_error: 23842.6641\n",
            "Epoch 385/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 15939.9307 - mean_absolute_error: 15939.9307\n",
            "Epoch 00385: val_loss did not improve from 19570.45703\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15691.2461 - mean_absolute_error: 15691.2461 - val_loss: 20293.1230 - val_mean_absolute_error: 20293.1230\n",
            "Epoch 386/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14432.5732 - mean_absolute_error: 14432.5732\n",
            "Epoch 00386: val_loss did not improve from 19570.45703\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14424.9336 - mean_absolute_error: 14424.9336 - val_loss: 20995.6387 - val_mean_absolute_error: 20995.6387\n",
            "Epoch 387/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17748.2148 - mean_absolute_error: 17748.2148\n",
            "Epoch 00387: val_loss did not improve from 19570.45703\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17758.3535 - mean_absolute_error: 17758.3535 - val_loss: 23346.8301 - val_mean_absolute_error: 23346.8301\n",
            "Epoch 388/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16217.9268 - mean_absolute_error: 16217.9268\n",
            "Epoch 00388: val_loss did not improve from 19570.45703\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15342.1045 - mean_absolute_error: 15342.1045 - val_loss: 19792.5703 - val_mean_absolute_error: 19792.5703\n",
            "Epoch 389/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16358.8018 - mean_absolute_error: 16358.8018\n",
            "Epoch 00389: val_loss did not improve from 19570.45703\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16358.8018 - mean_absolute_error: 16358.8018 - val_loss: 19756.3242 - val_mean_absolute_error: 19756.3242\n",
            "Epoch 390/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14441.4551 - mean_absolute_error: 14441.4551\n",
            "Epoch 00390: val_loss improved from 19570.45703 to 19274.31641, saving model to Weights-390--19274.31641.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 14752.4639 - mean_absolute_error: 14752.4639 - val_loss: 19274.3164 - val_mean_absolute_error: 19274.3164\n",
            "Epoch 391/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14717.1592 - mean_absolute_error: 14717.1592\n",
            "Epoch 00391: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15638.0156 - mean_absolute_error: 15638.0156 - val_loss: 23054.8047 - val_mean_absolute_error: 23054.8047\n",
            "Epoch 392/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15699.6514 - mean_absolute_error: 15699.6514\n",
            "Epoch 00392: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15427.4199 - mean_absolute_error: 15427.4199 - val_loss: 20575.8750 - val_mean_absolute_error: 20575.8750\n",
            "Epoch 393/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15501.6895 - mean_absolute_error: 15501.6895\n",
            "Epoch 00393: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15046.0732 - mean_absolute_error: 15046.0732 - val_loss: 20638.5664 - val_mean_absolute_error: 20638.5664\n",
            "Epoch 394/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15478.9707 - mean_absolute_error: 15478.9707\n",
            "Epoch 00394: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15437.1094 - mean_absolute_error: 15437.1094 - val_loss: 20279.1152 - val_mean_absolute_error: 20279.1152\n",
            "Epoch 395/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15312.1924 - mean_absolute_error: 15312.1924\n",
            "Epoch 00395: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14944.6592 - mean_absolute_error: 14944.6592 - val_loss: 22010.2598 - val_mean_absolute_error: 22010.2598\n",
            "Epoch 396/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14808.6992 - mean_absolute_error: 14808.6992\n",
            "Epoch 00396: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15263.4814 - mean_absolute_error: 15263.4814 - val_loss: 23643.0352 - val_mean_absolute_error: 23643.0352\n",
            "Epoch 397/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 15489.4932 - mean_absolute_error: 15489.4932\n",
            "Epoch 00397: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15307.2930 - mean_absolute_error: 15307.2930 - val_loss: 24213.9004 - val_mean_absolute_error: 24213.9004\n",
            "Epoch 398/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15758.9043 - mean_absolute_error: 15758.9043\n",
            "Epoch 00398: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15042.9932 - mean_absolute_error: 15042.9932 - val_loss: 20183.7852 - val_mean_absolute_error: 20183.7852\n",
            "Epoch 399/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 13624.4512 - mean_absolute_error: 13624.4512\n",
            "Epoch 00399: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14859.0117 - mean_absolute_error: 14859.0117 - val_loss: 25175.8809 - val_mean_absolute_error: 25175.8809\n",
            "Epoch 400/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 19124.9746 - mean_absolute_error: 19124.9746\n",
            "Epoch 00400: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 17098.4922 - mean_absolute_error: 17098.4922 - val_loss: 20127.3867 - val_mean_absolute_error: 20127.3867\n",
            "Epoch 401/500\n",
            "34/37 [==========================>...] - ETA: 0s - loss: 14850.2070 - mean_absolute_error: 14850.2070\n",
            "Epoch 00401: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 14727.8232 - mean_absolute_error: 14727.8232 - val_loss: 20117.1055 - val_mean_absolute_error: 20117.1055\n",
            "Epoch 402/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 16013.9980 - mean_absolute_error: 16013.9980\n",
            "Epoch 00402: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 16042.9268 - mean_absolute_error: 16042.9268 - val_loss: 21049.4727 - val_mean_absolute_error: 21049.4727\n",
            "Epoch 403/500\n",
            "18/37 [=============>................] - ETA: 0s - loss: 14432.7480 - mean_absolute_error: 14432.7480\n",
            "Epoch 00403: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14431.8730 - mean_absolute_error: 14431.8730 - val_loss: 20244.1758 - val_mean_absolute_error: 20244.1758\n",
            "Epoch 404/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 14999.6719 - mean_absolute_error: 14999.6719\n",
            "Epoch 00404: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 14900.9961 - mean_absolute_error: 14900.9961 - val_loss: 19993.7227 - val_mean_absolute_error: 19993.7227\n",
            "Epoch 405/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15035.4492 - mean_absolute_error: 15035.4492\n",
            "Epoch 00405: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15004.9590 - mean_absolute_error: 15004.9590 - val_loss: 19984.5625 - val_mean_absolute_error: 19984.5625\n",
            "Epoch 406/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14879.1846 - mean_absolute_error: 14879.1846\n",
            "Epoch 00406: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14760.7637 - mean_absolute_error: 14760.7637 - val_loss: 19953.8848 - val_mean_absolute_error: 19953.8848\n",
            "Epoch 407/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14331.0957 - mean_absolute_error: 14331.0957\n",
            "Epoch 00407: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14680.2793 - mean_absolute_error: 14680.2793 - val_loss: 20016.6035 - val_mean_absolute_error: 20016.6035\n",
            "Epoch 408/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15820.1797 - mean_absolute_error: 15820.1797\n",
            "Epoch 00408: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14774.9062 - mean_absolute_error: 14774.9062 - val_loss: 19278.7402 - val_mean_absolute_error: 19278.7402\n",
            "Epoch 409/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14268.6953 - mean_absolute_error: 14268.6953\n",
            "Epoch 00409: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14615.5615 - mean_absolute_error: 14615.5615 - val_loss: 19739.5957 - val_mean_absolute_error: 19739.5957\n",
            "Epoch 410/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15646.2217 - mean_absolute_error: 15646.2217\n",
            "Epoch 00410: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15788.0322 - mean_absolute_error: 15788.0322 - val_loss: 20259.7852 - val_mean_absolute_error: 20259.7852\n",
            "Epoch 411/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15555.3643 - mean_absolute_error: 15555.3643\n",
            "Epoch 00411: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15586.0840 - mean_absolute_error: 15586.0840 - val_loss: 21388.6992 - val_mean_absolute_error: 21388.6992\n",
            "Epoch 412/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 15468.3359 - mean_absolute_error: 15468.3359\n",
            "Epoch 00412: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15468.3359 - mean_absolute_error: 15468.3359 - val_loss: 20993.8965 - val_mean_absolute_error: 20993.8965\n",
            "Epoch 413/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14139.7783 - mean_absolute_error: 14139.7783\n",
            "Epoch 00413: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14686.7314 - mean_absolute_error: 14686.7314 - val_loss: 20301.2246 - val_mean_absolute_error: 20301.2246\n",
            "Epoch 414/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17758.8711 - mean_absolute_error: 17758.8711\n",
            "Epoch 00414: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17910.5176 - mean_absolute_error: 17910.5176 - val_loss: 21709.4785 - val_mean_absolute_error: 21709.4785\n",
            "Epoch 415/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15502.4346 - mean_absolute_error: 15502.4346\n",
            "Epoch 00415: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15925.4453 - mean_absolute_error: 15925.4453 - val_loss: 21922.0312 - val_mean_absolute_error: 21922.0312\n",
            "Epoch 416/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16600.3535 - mean_absolute_error: 16600.3535\n",
            "Epoch 00416: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16600.3535 - mean_absolute_error: 16600.3535 - val_loss: 20131.1855 - val_mean_absolute_error: 20131.1855\n",
            "Epoch 417/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14726.9766 - mean_absolute_error: 14726.9766\n",
            "Epoch 00417: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15062.6904 - mean_absolute_error: 15062.6904 - val_loss: 19584.6543 - val_mean_absolute_error: 19584.6543\n",
            "Epoch 418/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15440.5801 - mean_absolute_error: 15440.5801\n",
            "Epoch 00418: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14482.1748 - mean_absolute_error: 14482.1748 - val_loss: 19822.5234 - val_mean_absolute_error: 19822.5234\n",
            "Epoch 419/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14326.9268 - mean_absolute_error: 14326.9268\n",
            "Epoch 00419: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14921.1074 - mean_absolute_error: 14921.1074 - val_loss: 20771.3301 - val_mean_absolute_error: 20771.3301\n",
            "Epoch 420/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17320.8477 - mean_absolute_error: 17320.8477\n",
            "Epoch 00420: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17285.1484 - mean_absolute_error: 17285.1484 - val_loss: 21717.3496 - val_mean_absolute_error: 21717.3496\n",
            "Epoch 421/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15223.5410 - mean_absolute_error: 15223.5410\n",
            "Epoch 00421: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14757.4062 - mean_absolute_error: 14757.4062 - val_loss: 21175.6211 - val_mean_absolute_error: 21175.6211\n",
            "Epoch 422/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 15027.2695 - mean_absolute_error: 15027.2695\n",
            "Epoch 00422: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14476.2002 - mean_absolute_error: 14476.2002 - val_loss: 20326.9668 - val_mean_absolute_error: 20326.9668\n",
            "Epoch 423/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15516.7061 - mean_absolute_error: 15516.7061\n",
            "Epoch 00423: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14985.5732 - mean_absolute_error: 14985.5732 - val_loss: 20642.3789 - val_mean_absolute_error: 20642.3789\n",
            "Epoch 424/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14192.4629 - mean_absolute_error: 14192.4629\n",
            "Epoch 00424: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14513.7793 - mean_absolute_error: 14513.7793 - val_loss: 19812.1621 - val_mean_absolute_error: 19812.1621\n",
            "Epoch 425/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 13791.3604 - mean_absolute_error: 13791.3604\n",
            "Epoch 00425: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 13891.3682 - mean_absolute_error: 13891.3682 - val_loss: 19588.8965 - val_mean_absolute_error: 19588.8965\n",
            "Epoch 426/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 13912.6816 - mean_absolute_error: 13912.6816\n",
            "Epoch 00426: val_loss did not improve from 19274.31641\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 13990.2539 - mean_absolute_error: 13990.2539 - val_loss: 20240.8125 - val_mean_absolute_error: 20240.8125\n",
            "Epoch 427/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 12915.6074 - mean_absolute_error: 12915.6074\n",
            "Epoch 00427: val_loss improved from 19274.31641 to 19150.83594, saving model to Weights-427--19150.83594.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 13666.5000 - mean_absolute_error: 13666.5000 - val_loss: 19150.8359 - val_mean_absolute_error: 19150.8359\n",
            "Epoch 428/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 14955.9424 - mean_absolute_error: 14955.9424\n",
            "Epoch 00428: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15036.7090 - mean_absolute_error: 15036.7090 - val_loss: 19177.7012 - val_mean_absolute_error: 19177.7012\n",
            "Epoch 429/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14317.0732 - mean_absolute_error: 14317.0732\n",
            "Epoch 00429: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14962.3506 - mean_absolute_error: 14962.3506 - val_loss: 20044.3965 - val_mean_absolute_error: 20044.3965\n",
            "Epoch 430/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15051.1924 - mean_absolute_error: 15051.1924\n",
            "Epoch 00430: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14648.5312 - mean_absolute_error: 14648.5312 - val_loss: 20619.8262 - val_mean_absolute_error: 20619.8262\n",
            "Epoch 431/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 13526.7695 - mean_absolute_error: 13526.7695\n",
            "Epoch 00431: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14419.7451 - mean_absolute_error: 14419.7451 - val_loss: 22062.8613 - val_mean_absolute_error: 22062.8613\n",
            "Epoch 432/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17403.8320 - mean_absolute_error: 17403.8320\n",
            "Epoch 00432: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16403.6211 - mean_absolute_error: 16403.6211 - val_loss: 19994.6172 - val_mean_absolute_error: 19994.6172\n",
            "Epoch 433/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14018.6904 - mean_absolute_error: 14018.6904\n",
            "Epoch 00433: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 13941.3262 - mean_absolute_error: 13941.3262 - val_loss: 21077.0410 - val_mean_absolute_error: 21077.0410\n",
            "Epoch 434/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16580.0469 - mean_absolute_error: 16580.0469\n",
            "Epoch 00434: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16056.7412 - mean_absolute_error: 16056.7412 - val_loss: 21309.0430 - val_mean_absolute_error: 21309.0430\n",
            "Epoch 435/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14740.4951 - mean_absolute_error: 14740.4951\n",
            "Epoch 00435: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15421.3789 - mean_absolute_error: 15421.3789 - val_loss: 22880.3340 - val_mean_absolute_error: 22880.3340\n",
            "Epoch 436/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 15731.9014 - mean_absolute_error: 15731.9014\n",
            "Epoch 00436: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15753.3506 - mean_absolute_error: 15753.3506 - val_loss: 20088.4375 - val_mean_absolute_error: 20088.4375\n",
            "Epoch 437/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14526.9453 - mean_absolute_error: 14526.9453\n",
            "Epoch 00437: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14238.4326 - mean_absolute_error: 14238.4326 - val_loss: 20124.8984 - val_mean_absolute_error: 20124.8984\n",
            "Epoch 438/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 13594.9824 - mean_absolute_error: 13594.9824\n",
            "Epoch 00438: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15201.6494 - mean_absolute_error: 15201.6494 - val_loss: 27519.5352 - val_mean_absolute_error: 27519.5352\n",
            "Epoch 439/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 16493.1230 - mean_absolute_error: 16493.1230\n",
            "Epoch 00439: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16078.4863 - mean_absolute_error: 16078.4863 - val_loss: 20668.1445 - val_mean_absolute_error: 20668.1445\n",
            "Epoch 440/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 15642.1943 - mean_absolute_error: 15642.1943\n",
            "Epoch 00440: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15535.9980 - mean_absolute_error: 15535.9980 - val_loss: 20038.5215 - val_mean_absolute_error: 20038.5215\n",
            "Epoch 441/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 16840.5938 - mean_absolute_error: 16840.5938\n",
            "Epoch 00441: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16840.5938 - mean_absolute_error: 16840.5938 - val_loss: 21145.4434 - val_mean_absolute_error: 21145.4434\n",
            "Epoch 442/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16415.2949 - mean_absolute_error: 16415.2949\n",
            "Epoch 00442: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15868.4814 - mean_absolute_error: 15868.4814 - val_loss: 19224.4746 - val_mean_absolute_error: 19224.4746\n",
            "Epoch 443/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 14659.4844 - mean_absolute_error: 14659.4844\n",
            "Epoch 00443: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 14520.0977 - mean_absolute_error: 14520.0977 - val_loss: 19731.2168 - val_mean_absolute_error: 19731.2168\n",
            "Epoch 444/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 15272.5195 - mean_absolute_error: 15272.5195\n",
            "Epoch 00444: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15222.3096 - mean_absolute_error: 15222.3096 - val_loss: 20103.8242 - val_mean_absolute_error: 20103.8242\n",
            "Epoch 445/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 13897.9229 - mean_absolute_error: 13897.9229\n",
            "Epoch 00445: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14360.0410 - mean_absolute_error: 14360.0410 - val_loss: 19670.2305 - val_mean_absolute_error: 19670.2305\n",
            "Epoch 446/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 17002.4785 - mean_absolute_error: 17002.4785\n",
            "Epoch 00446: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17692.1699 - mean_absolute_error: 17692.1699 - val_loss: 20981.3398 - val_mean_absolute_error: 20981.3398\n",
            "Epoch 447/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 17044.8457 - mean_absolute_error: 17044.8457\n",
            "Epoch 00447: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 17044.8457 - mean_absolute_error: 17044.8457 - val_loss: 21389.5312 - val_mean_absolute_error: 21389.5312\n",
            "Epoch 448/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 17648.9316 - mean_absolute_error: 17648.9316\n",
            "Epoch 00448: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17648.9316 - mean_absolute_error: 17648.9316 - val_loss: 19690.1914 - val_mean_absolute_error: 19690.1914\n",
            "Epoch 449/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15721.7422 - mean_absolute_error: 15721.7422\n",
            "Epoch 00449: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14678.1006 - mean_absolute_error: 14678.1006 - val_loss: 21256.9121 - val_mean_absolute_error: 21256.9121\n",
            "Epoch 450/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 15735.0918 - mean_absolute_error: 15735.0918\n",
            "Epoch 00450: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14979.0273 - mean_absolute_error: 14979.0273 - val_loss: 20056.0020 - val_mean_absolute_error: 20056.0020\n",
            "Epoch 451/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 14811.2041 - mean_absolute_error: 14811.2041\n",
            "Epoch 00451: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14811.2041 - mean_absolute_error: 14811.2041 - val_loss: 19719.8379 - val_mean_absolute_error: 19719.8379\n",
            "Epoch 452/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14861.2207 - mean_absolute_error: 14861.2207\n",
            "Epoch 00452: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14837.8975 - mean_absolute_error: 14837.8975 - val_loss: 19369.4844 - val_mean_absolute_error: 19369.4844\n",
            "Epoch 453/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14104.9980 - mean_absolute_error: 14104.9980\n",
            "Epoch 00453: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14952.1885 - mean_absolute_error: 14952.1885 - val_loss: 19729.3555 - val_mean_absolute_error: 19729.3555\n",
            "Epoch 454/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14396.4512 - mean_absolute_error: 14396.4512\n",
            "Epoch 00454: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14540.3428 - mean_absolute_error: 14540.3428 - val_loss: 20374.6680 - val_mean_absolute_error: 20374.6680\n",
            "Epoch 455/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 14650.6455 - mean_absolute_error: 14650.6455\n",
            "Epoch 00455: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15714.3633 - mean_absolute_error: 15714.3633 - val_loss: 22805.6172 - val_mean_absolute_error: 22805.6172\n",
            "Epoch 456/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 18595.5879 - mean_absolute_error: 18595.5879\n",
            "Epoch 00456: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17790.6113 - mean_absolute_error: 17790.6113 - val_loss: 22570.1074 - val_mean_absolute_error: 22570.1074\n",
            "Epoch 457/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16174.7295 - mean_absolute_error: 16174.7295\n",
            "Epoch 00457: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15425.3271 - mean_absolute_error: 15425.3271 - val_loss: 22743.5508 - val_mean_absolute_error: 22743.5508\n",
            "Epoch 458/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 16220.7070 - mean_absolute_error: 16220.7070\n",
            "Epoch 00458: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15411.3594 - mean_absolute_error: 15411.3594 - val_loss: 20032.3906 - val_mean_absolute_error: 20032.3906\n",
            "Epoch 459/500\n",
            "18/37 [=============>................] - ETA: 0s - loss: 13877.2090 - mean_absolute_error: 13877.2090\n",
            "Epoch 00459: val_loss did not improve from 19150.83594\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14231.4170 - mean_absolute_error: 14231.4170 - val_loss: 23094.8457 - val_mean_absolute_error: 23094.8457\n",
            "Epoch 460/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15877.7207 - mean_absolute_error: 15877.7207\n",
            "Epoch 00460: val_loss improved from 19150.83594 to 18688.96484, saving model to Weights-460--18688.96484.hdf5\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15174.9180 - mean_absolute_error: 15174.9180 - val_loss: 18688.9648 - val_mean_absolute_error: 18688.9648\n",
            "Epoch 461/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 14532.7637 - mean_absolute_error: 14532.7637\n",
            "Epoch 00461: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14532.7637 - mean_absolute_error: 14532.7637 - val_loss: 21689.0352 - val_mean_absolute_error: 21689.0352\n",
            "Epoch 462/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15637.9873 - mean_absolute_error: 15637.9873\n",
            "Epoch 00462: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15636.3643 - mean_absolute_error: 15636.3643 - val_loss: 20501.4844 - val_mean_absolute_error: 20501.4844\n",
            "Epoch 463/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 13785.8232 - mean_absolute_error: 13785.8232\n",
            "Epoch 00463: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14743.1064 - mean_absolute_error: 14743.1064 - val_loss: 20849.6992 - val_mean_absolute_error: 20849.6992\n",
            "Epoch 464/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 13922.1367 - mean_absolute_error: 13922.1367\n",
            "Epoch 00464: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 13615.2939 - mean_absolute_error: 13615.2939 - val_loss: 19468.1172 - val_mean_absolute_error: 19468.1172\n",
            "Epoch 465/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 13705.9180 - mean_absolute_error: 13705.9180\n",
            "Epoch 00465: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14721.0205 - mean_absolute_error: 14721.0205 - val_loss: 20032.9590 - val_mean_absolute_error: 20032.9590\n",
            "Epoch 466/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14328.8076 - mean_absolute_error: 14328.8076\n",
            "Epoch 00466: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14399.6211 - mean_absolute_error: 14399.6211 - val_loss: 20163.7285 - val_mean_absolute_error: 20163.7285\n",
            "Epoch 467/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15716.6797 - mean_absolute_error: 15716.6797\n",
            "Epoch 00467: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15455.6426 - mean_absolute_error: 15455.6426 - val_loss: 19197.3965 - val_mean_absolute_error: 19197.3965\n",
            "Epoch 468/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 16004.6338 - mean_absolute_error: 16004.6338\n",
            "Epoch 00468: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15912.0273 - mean_absolute_error: 15912.0273 - val_loss: 19970.0430 - val_mean_absolute_error: 19970.0430\n",
            "Epoch 469/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 14531.6533 - mean_absolute_error: 14531.6533\n",
            "Epoch 00469: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 14547.7109 - mean_absolute_error: 14547.7109 - val_loss: 20325.9121 - val_mean_absolute_error: 20325.9121\n",
            "Epoch 470/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15693.1045 - mean_absolute_error: 15693.1045\n",
            "Epoch 00470: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15292.8643 - mean_absolute_error: 15292.8643 - val_loss: 21621.6289 - val_mean_absolute_error: 21621.6289\n",
            "Epoch 471/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16634.4141 - mean_absolute_error: 16634.4141\n",
            "Epoch 00471: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15420.9248 - mean_absolute_error: 15420.9248 - val_loss: 20407.3242 - val_mean_absolute_error: 20407.3242\n",
            "Epoch 472/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 14806.0820 - mean_absolute_error: 14806.0820\n",
            "Epoch 00472: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14417.5791 - mean_absolute_error: 14417.5791 - val_loss: 19633.9082 - val_mean_absolute_error: 19633.9082\n",
            "Epoch 473/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15485.4395 - mean_absolute_error: 15485.4395\n",
            "Epoch 00473: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16098.6641 - mean_absolute_error: 16098.6641 - val_loss: 24956.8438 - val_mean_absolute_error: 24956.8438\n",
            "Epoch 474/500\n",
            "36/37 [============================>.] - ETA: 0s - loss: 15108.4531 - mean_absolute_error: 15108.4531\n",
            "Epoch 00474: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 15118.3770 - mean_absolute_error: 15118.3770 - val_loss: 19260.2461 - val_mean_absolute_error: 19260.2461\n",
            "Epoch 475/500\n",
            "18/37 [=============>................] - ETA: 0s - loss: 17686.3691 - mean_absolute_error: 17686.3691\n",
            "Epoch 00475: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 17249.0137 - mean_absolute_error: 17249.0137 - val_loss: 19157.8711 - val_mean_absolute_error: 19157.8711\n",
            "Epoch 476/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14388.4707 - mean_absolute_error: 14388.4707\n",
            "Epoch 00476: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14676.5430 - mean_absolute_error: 14676.5430 - val_loss: 21125.3965 - val_mean_absolute_error: 21125.3965\n",
            "Epoch 477/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15381.5596 - mean_absolute_error: 15381.5596\n",
            "Epoch 00477: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15586.0137 - mean_absolute_error: 15586.0137 - val_loss: 19101.0645 - val_mean_absolute_error: 19101.0645\n",
            "Epoch 478/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 14265.7578 - mean_absolute_error: 14265.7578\n",
            "Epoch 00478: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14562.9883 - mean_absolute_error: 14562.9883 - val_loss: 19653.4551 - val_mean_absolute_error: 19653.4551\n",
            "Epoch 479/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15540.7793 - mean_absolute_error: 15540.7793\n",
            "Epoch 00479: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15716.1143 - mean_absolute_error: 15716.1143 - val_loss: 21221.9551 - val_mean_absolute_error: 21221.9551\n",
            "Epoch 480/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 13679.5059 - mean_absolute_error: 13679.5059\n",
            "Epoch 00480: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14509.4590 - mean_absolute_error: 14509.4590 - val_loss: 19884.7637 - val_mean_absolute_error: 19884.7637\n",
            "Epoch 481/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15193.3672 - mean_absolute_error: 15193.3672\n",
            "Epoch 00481: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14892.5908 - mean_absolute_error: 14892.5908 - val_loss: 20173.8613 - val_mean_absolute_error: 20173.8613\n",
            "Epoch 482/500\n",
            "37/37 [==============================] - ETA: 0s - loss: 13861.5020 - mean_absolute_error: 13861.5020\n",
            "Epoch 00482: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 13861.5020 - mean_absolute_error: 13861.5020 - val_loss: 19363.9316 - val_mean_absolute_error: 19363.9316\n",
            "Epoch 483/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 13752.3984 - mean_absolute_error: 13752.3984\n",
            "Epoch 00483: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 13772.3174 - mean_absolute_error: 13772.3174 - val_loss: 20436.2383 - val_mean_absolute_error: 20436.2383\n",
            "Epoch 484/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 18834.6445 - mean_absolute_error: 18834.6445\n",
            "Epoch 00484: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16992.3281 - mean_absolute_error: 16992.3281 - val_loss: 21317.7266 - val_mean_absolute_error: 21317.7266\n",
            "Epoch 485/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 13595.6299 - mean_absolute_error: 13595.6299\n",
            "Epoch 00485: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14155.1006 - mean_absolute_error: 14155.1006 - val_loss: 19892.5996 - val_mean_absolute_error: 19892.5996\n",
            "Epoch 486/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14058.3203 - mean_absolute_error: 14058.3203\n",
            "Epoch 00486: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14131.6475 - mean_absolute_error: 14131.6475 - val_loss: 19694.2480 - val_mean_absolute_error: 19694.2480\n",
            "Epoch 487/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 13175.1641 - mean_absolute_error: 13175.1641\n",
            "Epoch 00487: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14938.0342 - mean_absolute_error: 14938.0342 - val_loss: 20164.8125 - val_mean_absolute_error: 20164.8125\n",
            "Epoch 488/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 13963.0762 - mean_absolute_error: 13963.0762\n",
            "Epoch 00488: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14062.2510 - mean_absolute_error: 14062.2510 - val_loss: 19916.7031 - val_mean_absolute_error: 19916.7031\n",
            "Epoch 489/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14398.0098 - mean_absolute_error: 14398.0098\n",
            "Epoch 00489: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14410.9619 - mean_absolute_error: 14410.9619 - val_loss: 23155.4941 - val_mean_absolute_error: 23155.4941\n",
            "Epoch 490/500\n",
            "32/37 [========================>.....] - ETA: 0s - loss: 13736.9258 - mean_absolute_error: 13736.9258\n",
            "Epoch 00490: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 13809.6309 - mean_absolute_error: 13809.6309 - val_loss: 22033.0781 - val_mean_absolute_error: 22033.0781\n",
            "Epoch 491/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16692.7305 - mean_absolute_error: 16692.7305\n",
            "Epoch 00491: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16615.1172 - mean_absolute_error: 16615.1172 - val_loss: 19998.9609 - val_mean_absolute_error: 19998.9609\n",
            "Epoch 492/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 16708.7070 - mean_absolute_error: 16708.7070\n",
            "Epoch 00492: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16724.0859 - mean_absolute_error: 16724.0859 - val_loss: 19432.6602 - val_mean_absolute_error: 19432.6602\n",
            "Epoch 493/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 15024.9658 - mean_absolute_error: 15024.9658\n",
            "Epoch 00493: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14289.1172 - mean_absolute_error: 14289.1172 - val_loss: 19456.5684 - val_mean_absolute_error: 19456.5684\n",
            "Epoch 494/500\n",
            "21/37 [================>.............] - ETA: 0s - loss: 15836.1787 - mean_absolute_error: 15836.1787\n",
            "Epoch 00494: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15936.6182 - mean_absolute_error: 15936.6182 - val_loss: 23046.1660 - val_mean_absolute_error: 23046.1660\n",
            "Epoch 495/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 15132.9561 - mean_absolute_error: 15132.9561\n",
            "Epoch 00495: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 15303.2256 - mean_absolute_error: 15303.2256 - val_loss: 21129.0625 - val_mean_absolute_error: 21129.0625\n",
            "Epoch 496/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 17212.1836 - mean_absolute_error: 17212.1836\n",
            "Epoch 00496: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 16050.9756 - mean_absolute_error: 16050.9756 - val_loss: 19448.2637 - val_mean_absolute_error: 19448.2637\n",
            "Epoch 497/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 14316.8984 - mean_absolute_error: 14316.8984\n",
            "Epoch 00497: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14723.2930 - mean_absolute_error: 14723.2930 - val_loss: 19831.0000 - val_mean_absolute_error: 19831.0000\n",
            "Epoch 498/500\n",
            "35/37 [===========================>..] - ETA: 0s - loss: 13839.3389 - mean_absolute_error: 13839.3389\n",
            "Epoch 00498: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 4ms/step - loss: 13918.8701 - mean_absolute_error: 13918.8701 - val_loss: 19233.0840 - val_mean_absolute_error: 19233.0840\n",
            "Epoch 499/500\n",
            "20/37 [===============>..............] - ETA: 0s - loss: 14494.8984 - mean_absolute_error: 14494.8984\n",
            "Epoch 00499: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 13992.2051 - mean_absolute_error: 13992.2051 - val_loss: 19129.7070 - val_mean_absolute_error: 19129.7070\n",
            "Epoch 500/500\n",
            "19/37 [==============>...............] - ETA: 0s - loss: 13128.1006 - mean_absolute_error: 13128.1006\n",
            "Epoch 00500: val_loss did not improve from 18688.96484\n",
            "37/37 [==============================] - 0s 3ms/step - loss: 14552.4727 - mean_absolute_error: 14552.4727 - val_loss: 20399.6016 - val_mean_absolute_error: 20399.6016\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f50aacd0f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39sTlbBrdraH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load wights file of the best model :\n",
        "wights_file = 'Weights-460--18688.96484.hdf5' # choose the best checkpoint \n",
        "NN_model.load_weights(wights_file) # load it\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ecLK0Zp7WlE",
        "colab_type": "text"
      },
      "source": [
        "We see that the validation loss of the best model is 18688.96"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Y42dnt8evM",
        "colab_type": "text"
      },
      "source": [
        "## Fourth : Test the model\n",
        "We will submit the predictions on the test data to Kaggle and see how good our model is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06Wn845Se58B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_submission(prediction, sub_name):\n",
        "  my_submission = pd.DataFrame({'Id':pd.read_csv('test.csv').Id,'SalePrice':prediction})\n",
        "  my_submission.to_csv('{}.csv'.format(sub_name),index=False)\n",
        "  print('A submission file has been made')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xpq-iZkefCy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = NN_model.predict(test)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESo8GK6wfGG-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b35ae412-d8dc-4089-cc28-2214dcd6cad2"
      },
      "source": [
        "make_submission(predictions[:,0],'submission(NN).csv')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A submission file has been made\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vojrVZ9LEkgc",
        "colab_type": "text"
      },
      "source": [
        "## Fifth: Try another ML algorithms :\n",
        "Now, let us try another ML algorithm to compare the results.\n",
        "\n",
        "We will use random forest regressor and XGBRegressor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQwnJ4d-b1WK",
        "colab_type": "text"
      },
      "source": [
        "**Split training data to training and validation data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIyCwnGIfSoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X, val_X, train_y, val_y = train_test_split(train, target, test_size = 0.25, random_state = 14)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vHJ0gzZb_ri",
        "colab_type": "text"
      },
      "source": [
        "**We will try Random forest model first.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_8zI3N0fZsN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "8d3b97f5-6d93-4573-8716-967a4e11af91"
      },
      "source": [
        "model = RandomForestRegressor()\n",
        "model.fit(train_X,train_y)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
              "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                      max_samples=None, min_impurity_decrease=0.0,\n",
              "                      min_impurity_split=None, min_samples_leaf=1,\n",
              "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
              "                      random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5lsFUEPcGft",
        "colab_type": "text"
      },
      "source": [
        "**Get the mean absolute error on the validation data **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5aIXOOdfjH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a07070b-0cf0-456a-d208-5eebf5075548"
      },
      "source": [
        "predicted_prices = model.predict(val_X)\n",
        "MAE = mean_absolute_error(val_y , predicted_prices)\n",
        "print('Random forest validation MAE = ', MAE)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random forest validation MAE =  18036.218721461188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58hkLBfQfp5t",
        "colab_type": "text"
      },
      "source": [
        "Make a submission file and submit it to Kaggle to see the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3IFk9GWHP-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0815fc40-c566-453d-a045-7716c95728ef"
      },
      "source": [
        "predicted_prices = model.predict(test)\n",
        "make_submission(predicted_prices,'Submission(RF).csv')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A submission file has been made\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgZlYnjWcgqk",
        "colab_type": "text"
      },
      "source": [
        "**Now, let us try XGBoost model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO_BWLAIf9AU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "18246d7f-ea9e-4bd1-a2c3-b998ba5e9349"
      },
      "source": [
        "XGBModel = XGBRegressor()\n",
        "XGBModel.fit(train_X,train_y , verbose=False)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20:25:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
              "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
              "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
              "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "             silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSIqN9Pdc0Hr",
        "colab_type": "text"
      },
      "source": [
        "**Get the mean absolute error on the validation data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvzNYjNRgetU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5b2d9e2-22ce-4079-b317-2fa758c06dac"
      },
      "source": [
        "XGBpredictions = XGBModel.predict(val_X)\n",
        "MAE = mean_absolute_error(val_y , XGBpredictions)\n",
        "print('XGBoost validation MAE = ',MAE)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XGBoost validation MAE =  17869.75410958904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMh7f_2pc9rf",
        "colab_type": "text"
      },
      "source": [
        "**Make a submission file and submit it to Kaggle to see the result.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f20ozbxgkOL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5152e3e-86b9-4c4b-f8eb-f66a4c698988"
      },
      "source": [
        "XGBpredictions = XGBModel.predict(test)\n",
        "make_submission(XGBpredictions,'Submission(XGB).csv')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A submission file has been made\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}